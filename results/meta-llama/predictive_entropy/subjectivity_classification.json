[
  {
    "sample_idx": 0,
    "sentence": "Blanco established himself earlier in his career working for Dr. Luke's Kasz Money Productions.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9518903493881226
          ]
        ],
        "inference_time": 7.374994277954102
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9518903493881226
          ]
        ],
        "inference_time": 1.2860438823699951
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9518903493881226
          ]
        ],
        "inference_time": 1.2728710174560547
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9518903493881226
          ]
        ],
        "inference_time": 1.2330856323242188
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9518903493881226
          ]
        ],
        "inference_time": 1.21170973777771
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9518903493881226
          ]
        ],
        "inference_time": 1.2834560871124268
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9518903493881226
          ]
        ],
        "inference_time": 1.2799568176269531
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9518903493881226
          ]
        ],
        "inference_time": 1.2062456607818604
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9518903493881226
          ]
        ],
        "inference_time": 1.3699274063110352
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9518903493881226
          ]
        ],
        "inference_time": 1.218996524810791
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 1,
    "sentence": "RULE 13: ARTIFICIAL INTELLIGENCE  Not only this, but Gina also created an AI model of herself to achieve immortality.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8472078442573547
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.232499599456787
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8472078442573547
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3142871856689453
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.15279212594032288
          ]
        ],
        "inference_time": 1.28361177444458
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.15279212594032288
          ]
        ],
        "inference_time": 1.3434398174285889
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.15279212594032288
          ]
        ],
        "inference_time": 1.3444468975067139
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8472078442573547
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2681884765625
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8472078442573547
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.4008934497833252
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8472078442573547
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3783886432647705
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8472078442573547
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3828895092010498
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8472078442573547
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.418457269668579
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 2,
    "sentence": "The valuation is required by law and the figure is assessed independently by a pension specialist and has been reviewed by the National Audit Office.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.506636381149292
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2217223644256592
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.3377692699432373
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.352489948272705
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2560977935791016
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.220435619354248
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2942733764648438
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.3300983905792236
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.3361825942993164
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.3346655368804932
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 3,
    "sentence": "A sip can really hit the spot after a long bike ride or a walk.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.17091782059000948
          ]
        ],
        "inference_time": 1.2719314098358154
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7509869954698303
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.312579870223999
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.17091782059000948
          ]
        ],
        "inference_time": 1.212308406829834
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7509869954698303
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3042736053466797
      },
      {
        "repetition": 4,
        "generated_text": "objective\n",
        "token_probs": [
          [
            "objective",
            0.03436930850148201
          ],
          [
            "\n",
            0.3499464988708496
          ]
        ],
        "inference_time": 1.2689223289489746
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.17091782059000948
          ]
        ],
        "inference_time": 1.292065143585205
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7509869954698303
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.349583625793457
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7509869954698303
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2329771518707275
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7509869954698303
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2746472358703613
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7509869954698303
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2124414443969727
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 4,
    "sentence": "Lobster!\"",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "```python",
        "token_probs": [
          [
            "```",
            0.23372268676757812
          ],
          [
            "python",
            1.0
          ]
        ],
        "inference_time": 1.2700691223144531
      },
      {
        "repetition": 1,
        "generated_text": "Note:",
        "token_probs": [
          [
            "Note",
            0.17125923931598663
          ],
          [
            ":",
            1.0
          ]
        ],
        "inference_time": 1.258648157119751
      },
      {
        "repetition": 2,
        "generated_text": "Answer:",
        "token_probs": [
          [
            "Answer",
            0.026606915518641472
          ],
          [
            ":",
            1.0
          ]
        ],
        "inference_time": 1.2279276847839355
      },
      {
        "repetition": 3,
        "generated_text": "objective\n\n",
        "token_probs": [
          [
            "objective",
            0.13539272546768188
          ],
          [
            "\n\n",
            0.45590442419052124
          ]
        ],
        "inference_time": 1.1633896827697754
      },
      {
        "repetition": 4,
        "generated_text": "Objective\n",
        "token_probs": [
          [
            "Objective",
            0.22212781012058258
          ],
          [
            "\n",
            0.6382341384887695
          ]
        ],
        "inference_time": 1.3391454219818115
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.04426606285016277
          ]
        ],
        "inference_time": 1.2563109397888184
      },
      {
        "repetition": 6,
        "generated_text": "Objective\n",
        "token_probs": [
          [
            "Objective",
            0.22212781012058258
          ],
          [
            "\n",
            0.6382341384887695
          ]
        ],
        "inference_time": 1.328253984451294
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.12505055678446553
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.1613197326660156
      },
      {
        "repetition": 8,
        "generated_text": "```python",
        "token_probs": [
          [
            "```",
            0.23372268676757812
          ],
          [
            "python",
            1.0
          ]
        ],
        "inference_time": 1.219491958618164
      },
      {
        "repetition": 9,
        "generated_text": "Objective\n",
        "token_probs": [
          [
            "Objective",
            0.22212781012058258
          ],
          [
            "\n",
            0.6382341384887695
          ]
        ],
        "inference_time": 1.2300834655761719
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 5,
    "sentence": "But this is precisely the reason why Labour must reject the austerian urges that, inevitably, spring from the credit card analogy.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": " subject",
        "token_probs": [
          [
            " \"",
            0.03456968069076538
          ],
          [
            "subject",
            1.0
          ]
        ],
        "inference_time": 1.2715613842010498
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8737509250640869
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2410504817962646
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8737509250640869
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3593831062316895
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8737509250640869
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3425877094268799
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8737509250640869
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.323577642440796
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8737509250640869
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2943799495697021
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8737509250640869
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3430759906768799
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8737509250640869
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3023288249969482
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8737509250640869
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2725942134857178
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8737509250640869
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3253142833709717
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 6,
    "sentence": "Googled how to cook a good lobster and I read how hard it is to get it good because it can turn very tough, rubbery.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3234705924987793
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2986736297607422
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3739981651306152
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2317225933074951
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2556631565093994
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.298271656036377
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.350998878479004
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2339873313903809
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2957713603973389
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2729518413543701
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 7,
    "sentence": "Apartments cost from £392 per week to rent, which makes it more expensive than an average room in first-year halls (a single room without bathroom typically costs £250 per week) but more affordable than a lot of the postgraduate accommodation on offer.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.3301527500152588
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2989895343780518
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.255967140197754
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.400651216506958
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.3309986591339111
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2772555351257324
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2916042804718018
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2761693000793457
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.3109707832336426
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2557244300842285
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 8,
    "sentence": "We apologise to TikTok for not approaching it for comment prior to publication.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9624630212783813
          ]
        ],
        "inference_time": 1.2287068367004395
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9624630212783813
          ]
        ],
        "inference_time": 1.2777018547058105
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9624630212783813
          ]
        ],
        "inference_time": 1.2797892093658447
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9624630212783813
          ]
        ],
        "inference_time": 1.3091049194335938
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9624630212783813
          ]
        ],
        "inference_time": 1.214590311050415
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9624630212783813
          ]
        ],
        "inference_time": 1.3057622909545898
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9624630212783813
          ]
        ],
        "inference_time": 1.205925703048706
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9624630212783813
          ]
        ],
        "inference_time": 1.345623254776001
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9624630212783813
          ]
        ],
        "inference_time": 1.3476994037628174
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9624630212783813
          ]
        ],
        "inference_time": 1.2884573936462402
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 9,
    "sentence": "Crumbling parliament patched up with a few fig leaves  Parliament gained some new residents yesterday, and there are already questions as to their expenses.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.255384922027588
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.255197525024414
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.314527988433838
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3546981811523438
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3223497867584229
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3520410060882568
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.291656255722046
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3231537342071533
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.295147180557251
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.274923324584961
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 10,
    "sentence": "In 2019, he bagged a role in the ensemble movie Berlin, I Love You - which was labelled a 'empty, boring flop' by The Observer.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.34718430042266846
          ]
        ],
        "inference_time": 1.2465362548828125
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.34718430042266846
          ]
        ],
        "inference_time": 1.2901782989501953
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.34718430042266846
          ]
        ],
        "inference_time": 1.3076286315917969
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.6528157591819763
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2672460079193115
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.6528157591819763
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.249699592590332
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.6528157591819763
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3212604522705078
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.6528157591819763
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.265861988067627
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.6528157591819763
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3506622314453125
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.34718430042266846
          ]
        ],
        "inference_time": 1.280170202255249
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.6528157591819763
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2443861961364746
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 11,
    "sentence": "By the way, she is honestly the best cook.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": " subject",
        "token_probs": [
          [
            " \"",
            0.04604336991906166
          ],
          [
            "subject",
            1.0
          ]
        ],
        "inference_time": 1.335927963256836
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9539566040039062
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2923076152801514
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9539566040039062
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3419277667999268
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9539566040039062
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.276200771331787
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9539566040039062
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3070361614227295
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9539566040039062
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2206625938415527
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9539566040039062
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2482712268829346
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9539566040039062
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3117525577545166
      },
      {
        "repetition": 8,
        "generated_text": " subject",
        "token_probs": [
          [
            " \"",
            0.04604336991906166
          ],
          [
            "subject",
            1.0
          ]
        ],
        "inference_time": 1.2984545230865479
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9539566040039062
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3367595672607422
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 12,
    "sentence": "Families are doubling down on calls for perpetrators to be brought to justice, and say changes on the handling of femicide cases are necessary.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.822379469871521
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.22269606590271
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.822379469871521
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2441809177398682
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.17762050032615662
          ]
        ],
        "inference_time": 1.3050274848937988
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.17762050032615662
          ]
        ],
        "inference_time": 1.323106288909912
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.17762050032615662
          ]
        ],
        "inference_time": 1.2617430686950684
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.822379469871521
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2430524826049805
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.822379469871521
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2533113956451416
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.822379469871521
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.321626901626587
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.822379469871521
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3217127323150635
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.822379469871521
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.302459716796875
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 13,
    "sentence": "Anything we can actually do, we can afford.” Britain’s conundrum, today, is that the next government, whose job will be to fix the Tories’ mess, is led by politicians who share neither Keynes’s aims nor his innovative approach to public finance.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.48636531829833984
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.316091537475586
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.48636531829833984
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.290358543395996
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.4088074862957001
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3236446380615234
      },
      {
        "repetition": 3,
        "generated_text": "objective subjective",
        "token_probs": [
          [
            "objective",
            0.10482717305421829
          ],
          [
            " subjective",
            0.12773510813713074
          ]
        ],
        "inference_time": 1.272878885269165
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.4088074862957001
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2805447578430176
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.48636531829833984
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3645861148834229
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.48636531829833984
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.291358470916748
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.4088074862957001
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.364487886428833
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.4088074862957001
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2547504901885986
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.4088074862957001
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2806458473205566
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 14,
    "sentence": "“I just believe in being the best version of myself that I can possibly be, it makes me feel good.”  Read more real life stories  Not only does Gina swear by hydration - but she also has 13 other rules she sticks to like glue.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.4292648434638977
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.263622522354126
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.4386950433254242
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2948637008666992
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.4292648434638977
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3842029571533203
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.4386950433254242
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3527710437774658
      },
      {
        "repetition": 4,
        "generated_text": "“subject",
        "token_probs": [
          [
            "“",
            0.1320400983095169
          ],
          [
            "subject",
            1.0
          ]
        ],
        "inference_time": 1.3505117893218994
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.4292648434638977
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2963879108428955
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.4292648434638977
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2359206676483154
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.4292648434638977
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3865087032318115
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.4292648434638977
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3407444953918457
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.4386950433254242
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2847366333007812
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 15,
    "sentence": "“Russia’s dominance in the Black Sea is now challenged.”  The Boxing Day blast saw Vlad's valuable landing ship - docked in Crimea - turned into a raging fireball.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.33361655473709106
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2321445941925049
      },
      {
        "repetition": 1,
        "generated_text": "objective\n",
        "token_probs": [
          [
            "objective",
            0.1477866917848587
          ],
          [
            "\n",
            0.5418556332588196
          ]
        ],
        "inference_time": 1.296051263809204
      },
      {
        "repetition": 2,
        "generated_text": "“objective",
        "token_probs": [
          [
            "“",
            0.12020083516836166
          ],
          [
            "objective",
            0.9389709830284119
          ]
        ],
        "inference_time": 1.2458860874176025
      },
      {
        "repetition": 3,
        "generated_text": "“objective",
        "token_probs": [
          [
            "“",
            0.12020083516836166
          ],
          [
            "objective",
            0.9389709830284119
          ]
        ],
        "inference_time": 1.3797786235809326
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.33361655473709106
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3247649669647217
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.29550348225072653
          ]
        ],
        "inference_time": 1.3601865768432617
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.29550348225072653
          ]
        ],
        "inference_time": 1.2277119159698486
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.10289243163641881
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2725369930267334
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.29550348225072653
          ]
        ],
        "inference_time": 1.324312448501587
      },
      {
        "repetition": 9,
        "generated_text": "“objective",
        "token_probs": [
          [
            "“",
            0.12020083516836166
          ],
          [
            "objective",
            0.9389709830284119
          ]
        ],
        "inference_time": 1.3626124858856201
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 16,
    "sentence": "Sea drones & anti-ship missiles  The February 1 attack came after Ukraine unveiled its new underwater robot drone, a stealth Autonomous Underwater Vehicle (AUV).",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.226060152053833
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.3633413314819336
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.3125181198120117
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2696523666381836
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.3467915058135986
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.3297390937805176
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.349376916885376
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.233916997909546
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2300660610198975
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.3062851428985596
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 17,
    "sentence": "“They’re people who appreciate the onsite facilities such as the restaurant and gym and see living around older people as a lifestyle benefit rather than a hindrance,” she explains.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.3392902962195308
          ]
        ],
        "inference_time": 1.323134422302246
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.3392902962195308
          ]
        ],
        "inference_time": 1.2326650619506836
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.40404001893345054
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2734098434448242
      },
      {
        "repetition": 3,
        "generated_text": "“subject",
        "token_probs": [
          [
            "“",
            0.25666967034339905
          ],
          [
            "subject",
            0.4302796721458435
          ]
        ],
        "inference_time": 1.3260219097137451
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.3392902962195308
          ]
        ],
        "inference_time": 1.3094477653503418
      },
      {
        "repetition": 5,
        "generated_text": "“subject",
        "token_probs": [
          [
            "“",
            0.25666967034339905
          ],
          [
            "subject",
            0.4302796721458435
          ]
        ],
        "inference_time": 1.328284740447998
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.3392902962195308
          ]
        ],
        "inference_time": 1.3099312782287598
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.40404001893345054
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2595529556274414
      },
      {
        "repetition": 8,
        "generated_text": "“objective",
        "token_probs": [
          [
            "“",
            0.25666967034339905
          ],
          [
            "objective",
            0.5697203278541565
          ]
        ],
        "inference_time": 1.242156982421875
      },
      {
        "repetition": 9,
        "generated_text": "“objective",
        "token_probs": [
          [
            "“",
            0.25666967034339905
          ],
          [
            "objective",
            0.5697203278541565
          ]
        ],
        "inference_time": 1.2517659664154053
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 18,
    "sentence": "But none of this means that the ditched £28bn policy was optimal or, indeed, that an incoming chancellor can safely commit the Treasury to borrow and spend unlimited amounts.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8968284726142883
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2518184185028076
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8968284726142883
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.316222906112671
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8968284726142883
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.268195629119873
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8968284726142883
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2733471393585205
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8968284726142883
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.4109013080596924
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8968284726142883
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3461225032806396
      },
      {
        "repetition": 6,
        "generated_text": "```python",
        "token_probs": [
          [
            "```",
            0.10317151993513107
          ],
          [
            "python",
            1.0
          ]
        ],
        "inference_time": 1.3124780654907227
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8968284726142883
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2730224132537842
      },
      {
        "repetition": 8,
        "generated_text": "```python",
        "token_probs": [
          [
            "```",
            0.10317151993513107
          ],
          [
            "python",
            1.0
          ]
        ],
        "inference_time": 1.2395243644714355
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8968284726142883
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.333172082901001
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 19,
    "sentence": "The record, which topped out at 41 on the Billboard 200, included contributions from Bieber, Halsey, Calvin Harris, Omar Apollo and Gracie Abrams.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.3000156879425049
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2945001125335693
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2840375900268555
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.309913158416748
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2421088218688965
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.439310073852539
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2632219791412354
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.3171863555908203
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2669134140014648
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.290097951889038
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 20,
    "sentence": "The journalist Mike Smith was struck yesterday when he noticed",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective\n\n",
        "token_probs": [
          [
            "objective",
            0.11240144819021225
          ],
          [
            "\n\n",
            0.5941263437271118
          ]
        ],
        "inference_time": 1.2756330966949463
      },
      {
        "repetition": 1,
        "generated_text": "```python",
        "token_probs": [
          [
            "```",
            0.17979489266872406
          ],
          [
            "python",
            0.8626531958580017
          ]
        ],
        "inference_time": 1.254455327987671
      },
      {
        "repetition": 2,
        "generated_text": "```python",
        "token_probs": [
          [
            "```",
            0.17979489266872406
          ],
          [
            "python",
            0.8626531958580017
          ]
        ],
        "inference_time": 1.2416102886199951
      },
      {
        "repetition": 3,
        "generated_text": "Note:",
        "token_probs": [
          [
            "Note",
            0.3998422920703888
          ],
          [
            ":",
            1.0
          ]
        ],
        "inference_time": 1.341238021850586
      },
      {
        "repetition": 4,
        "generated_text": "objective\n\n",
        "token_probs": [
          [
            "objective",
            0.11240144819021225
          ],
          [
            "\n\n",
            0.5941263437271118
          ]
        ],
        "inference_time": 1.2776832580566406
      },
      {
        "repetition": 5,
        "generated_text": "Objective\n",
        "token_probs": [
          [
            "Objective",
            0.25745540857315063
          ],
          [
            "\n",
            0.685786783695221
          ]
        ],
        "inference_time": 1.3479607105255127
      },
      {
        "repetition": 6,
        "generated_text": "Objective\n",
        "token_probs": [
          [
            "Objective",
            0.25745540857315063
          ],
          [
            "\n",
            0.685786783695221
          ]
        ],
        "inference_time": 1.2145671844482422
      },
      {
        "repetition": 7,
        "generated_text": "```python",
        "token_probs": [
          [
            "```",
            0.17979489266872406
          ],
          [
            "python",
            0.8626531958580017
          ]
        ],
        "inference_time": 1.3145952224731445
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.044771920237911544
          ]
        ],
        "inference_time": 1.2016758918762207
      },
      {
        "repetition": 9,
        "generated_text": "Note:",
        "token_probs": [
          [
            "Note",
            0.3998422920703888
          ],
          [
            ":",
            1.0
          ]
        ],
        "inference_time": 1.3222272396087646
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 21,
    "sentence": "Two years later, he stepped into a leading man role once again when he appeared in The Other Me - about a architect who has an eye disease which enables him to see people's real motives.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.43533301069419394
          ]
        ],
        "inference_time": 1.37638521194458
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.4858325105201402
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.266998529434204
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.4858325105201402
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3115334510803223
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.43533301069419394
          ]
        ],
        "inference_time": 1.249143123626709
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.43533301069419394
          ]
        ],
        "inference_time": 1.3258984088897705
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.4858325105201402
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3080964088439941
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.43533301069419394
          ]
        ],
        "inference_time": 1.3161423206329346
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.43533301069419394
          ]
        ],
        "inference_time": 1.3281364440917969
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.43533301069419394
          ]
        ],
        "inference_time": 1.3031690120697021
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.43533301069419394
          ]
        ],
        "inference_time": 1.2615482807159424
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 22,
    "sentence": "At the time, Time magazine dubbed the film 'disjoined' - saying that 'characters that Nicholls brought so cunningly to life in the book feel rushed through a timeline, tied to an agenda'.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective\n",
        "token_probs": [
          [
            "objective",
            0.05321437492966652
          ],
          [
            "\n",
            0.33226919174194336
          ]
        ],
        "inference_time": 1.2534677982330322
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9467856287956238
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2528228759765625
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9467856287956238
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2618427276611328
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9467856287956238
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2708702087402344
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9467856287956238
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2483570575714111
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9467856287956238
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.392482042312622
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9467856287956238
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3286097049713135
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9467856287956238
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.360727310180664
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9467856287956238
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2312302589416504
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9467856287956238
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3027310371398926
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 23,
    "sentence": "“We were in shock,” says Wangari’s father.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.1557478373291188
          ]
        ],
        "inference_time": 1.2807056903839111
      },
      {
        "repetition": 1,
        "generated_text": "“objective",
        "token_probs": [
          [
            "“",
            0.5419084429740906
          ],
          [
            "objective",
            0.5117485523223877
          ]
        ],
        "inference_time": 1.2402739524841309
      },
      {
        "repetition": 2,
        "generated_text": "“objective",
        "token_probs": [
          [
            "“",
            0.5419084429740906
          ],
          [
            "objective",
            0.5117485523223877
          ]
        ],
        "inference_time": 1.214540719985962
      },
      {
        "repetition": 3,
        "generated_text": "“subject",
        "token_probs": [
          [
            "“",
            0.5419084429740906
          ],
          [
            "subject",
            0.4882514774799347
          ]
        ],
        "inference_time": 1.2807421684265137
      },
      {
        "repetition": 4,
        "generated_text": "“objective",
        "token_probs": [
          [
            "“",
            0.5419084429740906
          ],
          [
            "objective",
            0.5117485523223877
          ]
        ],
        "inference_time": 1.2261419296264648
      },
      {
        "repetition": 5,
        "generated_text": "“objective",
        "token_probs": [
          [
            "“",
            0.5419084429740906
          ],
          [
            "objective",
            0.5117485523223877
          ]
        ],
        "inference_time": 1.3086254596710205
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.30234373584691987
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.196385145187378
      },
      {
        "repetition": 7,
        "generated_text": "“objective",
        "token_probs": [
          [
            "“",
            0.5419084429740906
          ],
          [
            "objective",
            0.5117485523223877
          ]
        ],
        "inference_time": 1.320857048034668
      },
      {
        "repetition": 8,
        "generated_text": "“objective",
        "token_probs": [
          [
            "“",
            0.5419084429740906
          ],
          [
            "objective",
            0.5117485523223877
          ]
        ],
        "inference_time": 1.2995531558990479
      },
      {
        "repetition": 9,
        "generated_text": "“objective",
        "token_probs": [
          [
            "“",
            0.5419084429740906
          ],
          [
            "objective",
            0.5117485523223877
          ]
        ],
        "inference_time": 1.341296911239624
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 24,
    "sentence": "House Democrats and the remaining pro-Ukraine House Republicans are casting about behind the scenes for a solution.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.8477790951728821
          ]
        ],
        "inference_time": 1.237985610961914
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.1522209495306015
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2623910903930664
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.8477790951728821
          ]
        ],
        "inference_time": 1.296654224395752
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.1522209495306015
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3377435207366943
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.8477790951728821
          ]
        ],
        "inference_time": 1.297849178314209
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.8477790951728821
          ]
        ],
        "inference_time": 1.2873649597167969
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.8477790951728821
          ]
        ],
        "inference_time": 1.2242488861083984
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.8477790951728821
          ]
        ],
        "inference_time": 1.3063700199127197
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.8477790951728821
          ]
        ],
        "inference_time": 1.2904307842254639
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.8477790951728821
          ]
        ],
        "inference_time": 1.3221139907836914
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 25,
    "sentence": "Austerity, and the credit card analogy that provides its thin veneer of logic, is not just bad for workers and people in desperate need of state support during tough times; it also depresses investment.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.1772388368844986
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2698333263397217
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.1772388368844986
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2954378128051758
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7362083196640015
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.4162607192993164
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7362083196640015
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3644096851348877
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.1772388368844986
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3244447708129883
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7362083196640015
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.294929027557373
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7362083196640015
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2756612300872803
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7362083196640015
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.366419792175293
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7362083196640015
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.229720115661621
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7362083196640015
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3375160694122314
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 26,
    "sentence": "In a pre-dawn vote on Tuesday, Graham joined the majority of Senate Republicans in opposing a foreign aid package that would rush wartime assistance to Ukraine as it approaches the second anniversary of Russia’s full invasion.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.3361384868621826
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.3540124893188477
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2465672492980957
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.3847324848175049
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2947778701782227
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2770326137542725
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.277963638305664
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2507030963897705
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.3035683631896973
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2789032459259033
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 27,
    "sentence": "The Observer's Philip French dubbed it 'thin, superficial and sentimental' and said the casting of Anne Hathway was 'disastrous'.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2512712478637695
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2437875270843506
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2433154582977295
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3617289066314697
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.326157569885254
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3372094631195068
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2224998474121094
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2644102573394775
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.315333604812622
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.360435962677002
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 28,
    "sentence": "A third commented: \"$20 for $6k - not bad!\"",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.5049246393630469
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2042326927185059
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.16926767001653076
          ]
        ],
        "inference_time": 1.2841174602508545
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.16926767001653076
          ]
        ],
        "inference_time": 1.2830357551574707
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.5049246393630469
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3251686096191406
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.5049246393630469
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.27573823928833
      },
      {
        "repetition": 5,
        "generated_text": " subject",
        "token_probs": [
          [
            " \"",
            0.06322716921567917
          ],
          [
            "subject",
            0.8229913115501404
          ]
        ],
        "inference_time": 1.2539758682250977
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.5049246393630469
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2871289253234863
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.5049246393630469
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2512717247009277
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.5049246393630469
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2800495624542236
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.16926767001653076
          ]
        ],
        "inference_time": 1.3777360916137695
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 29,
    "sentence": "The plan incorporates cash payments supplemented by contingent contributions.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.213965654373169
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.3362886905670166
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2143487930297852
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.318044662475586
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2715132236480713
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.256220817565918
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2832512855529785
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2644774913787842
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2486188411712646
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.345334768295288
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 30,
    "sentence": "It is true that the Tories will leave scorched earth behind for the next government, with a budget dripping in red ink and a pitiful level of investment in the technologies and services the UK needs to escape a long-term slump.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2917907238006592
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3388252258300781
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2346937656402588
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3156695365905762
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3340022563934326
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3501999378204346
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.318817377090454
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3431546688079834
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3219540119171143
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3514325618743896
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 31,
    "sentence": "A WOMAN who has been dubbed ‘the world’s hottest gran’ has revealed how she stays looking eternally young at the age of 53.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7083902955055237
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2977876663208008
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7083902955055237
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.240614652633667
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7083902955055237
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2752468585968018
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.2916097044944763
          ]
        ],
        "inference_time": 1.3029437065124512
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7083902955055237
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.322300672531128
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.2916097044944763
          ]
        ],
        "inference_time": 1.3533060550689697
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.2916097044944763
          ]
        ],
        "inference_time": 1.245926856994629
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.2916097044944763
          ]
        ],
        "inference_time": 1.2315983772277832
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7083902955055237
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2653133869171143
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7083902955055237
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3214669227600098
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 32,
    "sentence": "Couldn't say everything I wanted to in the video.\"",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.779994547367096
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.248422384262085
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.779994547367096
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2246358394622803
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.779994547367096
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2741529941558838
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.779994547367096
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.306469440460205
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.779994547367096
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3152186870574951
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.779994547367096
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.339249610900879
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.779994547367096
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2401132583618164
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.779994547367096
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.234236717224121
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.779994547367096
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2370953559875488
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.22000546753406525
          ]
        ],
        "inference_time": 1.3126192092895508
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 33,
    "sentence": "From the Senate floor, Senator Mitch McConnell, the top Republican, delivered increasingly urgent pleas for his conference to rise to the occasion and support America’s allies, even after his plan to tie border security to foreign aid collapsed, torpedoed by Trump’s opposition.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8353064656257629
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3439102172851562
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8353064656257629
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2939057350158691
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8353064656257629
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3458173274993896
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8353064656257629
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.281872034072876
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8353064656257629
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.312617540359497
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8353064656257629
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3849000930786133
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8353064656257629
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2594032287597656
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8353064656257629
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.359004259109497
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8353064656257629
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2691540718078613
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.16469348967075348
          ]
        ],
        "inference_time": 1.3468477725982666
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 34,
    "sentence": "The Hollywood Reporter was particularly gushing of Jim Sturgess' performance - saying the actor had 'staked his claim as the new Hugh Grant only without the fussy mannerisms'.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.951802134513855
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2883005142211914
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.951802134513855
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3246121406555176
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.951802134513855
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2291297912597656
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.951802134513855
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.331251621246338
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.951802134513855
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.326418161392212
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.951802134513855
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3388655185699463
      },
      {
        "repetition": 6,
        "generated_text": "```python",
        "token_probs": [
          [
            "```",
            0.04819789528846741
          ],
          [
            "python",
            1.0
          ]
        ],
        "inference_time": 1.33058500289917
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.951802134513855
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2847683429718018
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.951802134513855
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2420976161956787
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.951802134513855
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3181848526000977
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 35,
    "sentence": "The Neptune \"super missile\", revealed in August last year, was reportedly snatched from behind enemy lines during a raid on Putin's prized £200million air defence system.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.6883354783058167
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3104186058044434
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.6883354783058167
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3718366622924805
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.6883354783058167
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.254049301147461
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.6883354783058167
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2513928413391113
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.31166455149650574
          ]
        ],
        "inference_time": 1.2847459316253662
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.31166455149650574
          ]
        ],
        "inference_time": 1.3389391899108887
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.6883354783058167
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.304581642150879
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.6883354783058167
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2523431777954102
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.6883354783058167
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3089590072631836
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.6883354783058167
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2468745708465576
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 36,
    "sentence": "- Actor was on track to become a Hollywood star when he appeared in One Day  - Read More: How Taylor Swift's savvy marketing executive mother Andrea turned her daughter into a superstar  David Nicholls' romantic novel One Day became an overnight fan favourite when it was first released in 2009 and is now being rediscovered by a new audience thanks to Netflix's adaptation.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.7478985943165952
          ]
        ],
        "inference_time": 1.3652973175048828
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.12365803236752448
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3907036781311035
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.7478985943165952
          ]
        ],
        "inference_time": 1.4411389827728271
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.12365803236752448
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.312629222869873
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.7478985943165952
          ]
        ],
        "inference_time": 1.3591861724853516
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.7478985943165952
          ]
        ],
        "inference_time": 1.287677526473999
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.7478985943165952
          ]
        ],
        "inference_time": 1.356520414352417
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.12365803236752448
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.394127368927002
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.7478985943165952
          ]
        ],
        "inference_time": 1.3676097393035889
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.7478985943165952
          ]
        ],
        "inference_time": 1.3828656673431396
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 37,
    "sentence": "\"$6k is a lot of money,\" wrote one.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "Objective/",
        "token_probs": [
          [
            "Objective",
            0.055979445576667786
          ],
          [
            "/",
            0.057661060243844986
          ]
        ],
        "inference_time": 1.2981455326080322
      },
      {
        "repetition": 1,
        "generated_text": "```\n",
        "token_probs": [
          [
            "``",
            0.0417618602514267
          ],
          [
            "`\n",
            1.0
          ]
        ],
        "inference_time": 1.2194087505340576
      },
      {
        "repetition": 2,
        "generated_text": "```python",
        "token_probs": [
          [
            "```",
            0.07923693209886551
          ],
          [
            "python",
            1.0
          ]
        ],
        "inference_time": 1.3151893615722656
      },
      {
        "repetition": 3,
        "generated_text": "```\n",
        "token_probs": [
          [
            "``",
            0.0417618602514267
          ],
          [
            "`\n",
            1.0
          ]
        ],
        "inference_time": 1.228771686553955
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.5677549567271178
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.325312614440918
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.5677549567271178
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3240101337432861
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.5677549567271178
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.270035743713379
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.09288448843948593
          ]
        ],
        "inference_time": 1.22758150100708
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.5677549567271178
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2402150630950928
      },
      {
        "repetition": 9,
        "generated_text": "```python",
        "token_probs": [
          [
            "```",
            0.07923693209886551
          ],
          [
            "python",
            1.0
          ]
        ],
        "inference_time": 1.2877423763275146
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 38,
    "sentence": "He told The Independent in 2021: 'If someone does a bad one, I can’t watch the film.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8219482898712158
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3540549278259277
      },
      {
        "repetition": 1,
        "generated_text": "```python",
        "token_probs": [
          [
            "```",
            0.09326101839542389
          ],
          [
            "python",
            1.0
          ]
        ],
        "inference_time": 1.2481040954589844
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8219482898712158
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.358945369720459
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8219482898712158
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.254976749420166
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8219482898712158
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3452718257904053
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8219482898712158
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.33127760887146
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8219482898712158
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2868800163269043
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8219482898712158
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3232111930847168
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8219482898712158
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2630836963653564
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8219482898712158
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3208413124084473
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 39,
    "sentence": "Gina Stewart has previously hit the headlines for her youthful appearance and most recently, immortalising herself as an AI model.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.550018310546875
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3260772228240967
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.550018310546875
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.4014644622802734
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.550018310546875
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.4303996562957764
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.550018310546875
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.5674993991851807
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.550018310546875
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2723925113677979
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.449981689453125
          ]
        ],
        "inference_time": 1.240363597869873
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.449981689453125
          ]
        ],
        "inference_time": 1.332627296447754
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.550018310546875
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2716076374053955
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.449981689453125
          ]
        ],
        "inference_time": 1.3482046127319336
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.550018310546875
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2501814365386963
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 40,
    "sentence": "Anne is a very warm actress.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.5404207110404968
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2001245021820068
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.5404207110404968
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.1565420627593994
      },
      {
        "repetition": 2,
        "generated_text": "```sql",
        "token_probs": [
          [
            "```",
            0.30428627133369446
          ],
          [
            "sql",
            0.10489241778850555
          ]
        ],
        "inference_time": 1.1883623600006104
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.5404207110404968
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3081653118133545
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.05776406452059746
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2001152038574219
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.5404207110404968
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.1514992713928223
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.5404207110404968
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.259078025817871
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.5404207110404968
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.15090012550354
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.5404207110404968
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.1598763465881348
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.5404207110404968
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.1398060321807861
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 41,
    "sentence": "And I was good at it.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "Note:",
        "token_probs": [
          [
            "Note",
            0.02230353094637394
          ],
          [
            ":",
            1.0
          ]
        ],
        "inference_time": 1.204519271850586
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.3802712234693697
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2239694595336914
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.3802712234693697
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.144897699356079
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.3802712234693697
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.1444745063781738
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.3802712234693697
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2733724117279053
      },
      {
        "repetition": 5,
        "generated_text": "objective\n",
        "token_probs": [
          [
            "objective",
            0.11599895358085632
          ],
          [
            "\n",
            0.3745100498199463
          ]
        ],
        "inference_time": 1.2157611846923828
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.3802712234693697
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2034416198730469
      },
      {
        "repetition": 7,
        "generated_text": "objective\n",
        "token_probs": [
          [
            "objective",
            0.11599895358085632
          ],
          [
            "\n",
            0.3745100498199463
          ]
        ],
        "inference_time": 1.1960296630859375
      },
      {
        "repetition": 8,
        "generated_text": "Objective:",
        "token_probs": [
          [
            "Objective",
            0.05594928190112114
          ],
          [
            ":",
            0.08811558037996292
          ]
        ],
        "inference_time": 1.1558516025543213
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.3802712234693697
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.1804654598236084
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 42,
    "sentence": "“On matters like femicide which society takes lightly, you don’t just get justice,” says Kamande.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2325291633605957
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3350419998168945
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3344261646270752
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2742335796356201
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2913925647735596
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3354403972625732
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3356685638427734
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3365602493286133
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2991087436676025
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2721459865570068
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 43,
    "sentence": "hosted by Laura Rangeley and Michael Deakin).",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "```\n",
        "token_probs": [
          [
            "``",
            0.02647782675921917
          ],
          [
            "`\n",
            1.0
          ]
        ],
        "inference_time": 1.296137809753418
      },
      {
        "repetition": 1,
        "generated_text": "```python",
        "token_probs": [
          [
            "```",
            0.5173293352127075
          ],
          [
            "python",
            0.9020720720291138
          ]
        ],
        "inference_time": 1.2438201904296875
      },
      {
        "repetition": 2,
        "generated_text": "```python",
        "token_probs": [
          [
            "```",
            0.5173293352127075
          ],
          [
            "python",
            0.9020720720291138
          ]
        ],
        "inference_time": 1.26332688331604
      },
      {
        "repetition": 3,
        "generated_text": " objective",
        "token_probs": [
          [
            " \"",
            0.025589406490325928
          ],
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.3641784191131592
      },
      {
        "repetition": 4,
        "generated_text": "```python",
        "token_probs": [
          [
            "```",
            0.5173293352127075
          ],
          [
            "python",
            0.9020720720291138
          ]
        ],
        "inference_time": 1.223412275314331
      },
      {
        "repetition": 5,
        "generated_text": "```python",
        "token_probs": [
          [
            "```",
            0.5173293352127075
          ],
          [
            "python",
            0.9020720720291138
          ]
        ],
        "inference_time": 1.2827563285827637
      },
      {
        "repetition": 6,
        "generated_text": "```python",
        "token_probs": [
          [
            "```",
            0.5173293352127075
          ],
          [
            "python",
            0.9020720720291138
          ]
        ],
        "inference_time": 1.2046396732330322
      },
      {
        "repetition": 7,
        "generated_text": "Note:",
        "token_probs": [
          [
            "Note",
            0.06835781782865524
          ],
          [
            ":",
            1.0
          ]
        ],
        "inference_time": 1.3453545570373535
      },
      {
        "repetition": 8,
        "generated_text": "Objective:",
        "token_probs": [
          [
            "Objective",
            0.16120538115501404
          ],
          [
            ":",
            0.11443133652210236
          ]
        ],
        "inference_time": 1.3103423118591309
      },
      {
        "repetition": 9,
        "generated_text": "```python",
        "token_probs": [
          [
            "```",
            0.5173293352127075
          ],
          [
            "python",
            0.9020720720291138
          ]
        ],
        "inference_time": 1.2523396015167236
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 44,
    "sentence": "Following One Day, the actor was dubbed the 'new Hugh Grant'  Pictured: Jim Sturgess - who is now a musician - opposite David Jason in A Touch of Frost in March 2003  Speaking to The Telegraph at the time, Jim - who starred in the Beatles-inspired movie Across the Universe beforehand - admitted that he hadn't read the book when he had his first audition.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective\n\n",
        "token_probs": [
          [
            "objective",
            0.2624669373035431
          ],
          [
            "\n\n",
            0.6295455098152161
          ]
        ],
        "inference_time": 1.3946659564971924
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.11908144503831863
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.34035062789917
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.2546326616490866
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3141283988952637
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.36381899326195466
          ]
        ],
        "inference_time": 1.400068759918213
      },
      {
        "repetition": 4,
        "generated_text": "objective\n",
        "token_probs": [
          [
            "objective",
            0.2624669373035431
          ],
          [
            "\n",
            0.37045446038246155
          ]
        ],
        "inference_time": 1.3281655311584473
      },
      {
        "repetition": 5,
        "generated_text": "objective\n\n",
        "token_probs": [
          [
            "objective",
            0.2624669373035431
          ],
          [
            "\n\n",
            0.6295455098152161
          ]
        ],
        "inference_time": 1.4000320434570312
      },
      {
        "repetition": 6,
        "generated_text": "objective\n\n",
        "token_probs": [
          [
            "objective",
            0.2624669373035431
          ],
          [
            "\n\n",
            0.6295455098152161
          ]
        ],
        "inference_time": 1.3200798034667969
      },
      {
        "repetition": 7,
        "generated_text": "objective\n",
        "token_probs": [
          [
            "objective",
            0.2624669373035431
          ],
          [
            "\n",
            0.37045446038246155
          ]
        ],
        "inference_time": 1.3478639125823975
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.2546326616490866
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.308953046798706
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.36381899326195466
          ]
        ],
        "inference_time": 1.3984401226043701
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 45,
    "sentence": "He has collaborated with a bevy of big name artists - including Gomez herself, on tracks such as 2023's Single Soon, and his 2019 song I Can’t Get Enough.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.8015246391296387
          ]
        ],
        "inference_time": 1.3283970355987549
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.8015246391296387
          ]
        ],
        "inference_time": 1.327773094177246
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.8015246391296387
          ]
        ],
        "inference_time": 1.2586383819580078
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.8015246391296387
          ]
        ],
        "inference_time": 1.404813528060913
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.8015246391296387
          ]
        ],
        "inference_time": 1.2665832042694092
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.8015246391296387
          ]
        ],
        "inference_time": 1.2581984996795654
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.8015246391296387
          ]
        ],
        "inference_time": 1.2881667613983154
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.8015246391296387
          ]
        ],
        "inference_time": 1.2512898445129395
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.8015246391296387
          ]
        ],
        "inference_time": 1.318099021911621
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.8015246391296387
          ]
        ],
        "inference_time": 1.2275159358978271
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 46,
    "sentence": "RULE 4: MOISTURISE  Gina uses organic coconut oil every day on her body, as she admitted \"I have moisturised my entire body every day since I was 19.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.05694730952382088
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2808396816253662
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.4604535059419881
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3076012134552002
      },
      {
        "repetition": 2,
        "generated_text": "```python",
        "token_probs": [
          [
            "```",
            0.3712991774082184
          ],
          [
            "python",
            1.0
          ]
        ],
        "inference_time": 1.3245868682861328
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.4604535059419881
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3263499736785889
      },
      {
        "repetition": 4,
        "generated_text": "```python",
        "token_probs": [
          [
            "```",
            0.3712991774082184
          ],
          [
            "python",
            1.0
          ]
        ],
        "inference_time": 1.3133163452148438
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.4604535059419881
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.247971534729004
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.4604535059419881
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2376916408538818
      },
      {
        "repetition": 7,
        "generated_text": "objective\n\n",
        "token_probs": [
          [
            "objective",
            0.05843481793999672
          ],
          [
            "\n\n",
            0.6852738261222839
          ]
        ],
        "inference_time": 1.3352630138397217
      },
      {
        "repetition": 8,
        "generated_text": "objective\n\n",
        "token_probs": [
          [
            "objective",
            0.05843481793999672
          ],
          [
            "\n\n",
            0.6852738261222839
          ]
        ],
        "inference_time": 1.3309729099273682
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.4604535059419881
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.4002788066864014
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 47,
    "sentence": "It’s this second group that Barratt is targeting for Ayrton House – final year medical students and graduate trainees from the surrounding universities, which include Westminster, Middlesex and UCL.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.248633623123169
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2507054805755615
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2492945194244385
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2767691612243652
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.245328664779663
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.257476806640625
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2203717231750488
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.273533582687378
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2221620082855225
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2563128471374512
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 48,
    "sentence": "This is why the more Osborne slashed public spending in the 2010s, the more money he needed to borrow.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2630305290222168
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2648191452026367
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2259857654571533
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.27536940574646
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2284607887268066
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.249021291732788
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2909846305847168
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2190942764282227
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2665798664093018
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2280304431915283
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 49,
    "sentence": "When your credit card is “maxed out”, you do indeed need immediately to tighten your belt.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.6980470327371364
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2553794384002686
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.6980470327371364
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2161719799041748
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.6980470327371364
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2554922103881836
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.23610831615058103
          ]
        ],
        "inference_time": 1.230210542678833
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.23610831615058103
          ]
        ],
        "inference_time": 1.2603974342346191
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.6980470327371364
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.220475673675537
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.6980470327371364
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.256397008895874
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.6980470327371364
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.226799726486206
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.6980470327371364
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2967541217803955
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.6980470327371364
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.21458101272583
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 50,
    "sentence": "This was inaccurate.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.15084730088710785
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.1868541240692139
      },
      {
        "repetition": 1,
        "generated_text": "This was",
        "token_probs": [
          [
            "This",
            0.09086239337921143
          ],
          [
            " was",
            0.8120035529136658
          ]
        ],
        "inference_time": 1.161877155303955
      },
      {
        "repetition": 2,
        "generated_text": "```python",
        "token_probs": [
          [
            "```",
            0.3531726002693176
          ],
          [
            "python",
            1.0
          ]
        ],
        "inference_time": 1.1745619773864746
      },
      {
        "repetition": 3,
        "generated_text": "```python",
        "token_probs": [
          [
            "```",
            0.3531726002693176
          ],
          [
            "python",
            1.0
          ]
        ],
        "inference_time": 1.1680173873901367
      },
      {
        "repetition": 4,
        "generated_text": "```python",
        "token_probs": [
          [
            "```",
            0.3531726002693176
          ],
          [
            "python",
            1.0
          ]
        ],
        "inference_time": 1.1422662734985352
      },
      {
        "repetition": 5,
        "generated_text": "This was",
        "token_probs": [
          [
            "This",
            0.09086239337921143
          ],
          [
            " was",
            0.8120035529136658
          ]
        ],
        "inference_time": 1.155026912689209
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.15084730088710785
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.147995948791504
      },
      {
        "repetition": 7,
        "generated_text": "```python",
        "token_probs": [
          [
            "```",
            0.3531726002693176
          ],
          [
            "python",
            1.0
          ]
        ],
        "inference_time": 1.1481778621673584
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.15084730088710785
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.1706833839416504
      },
      {
        "repetition": 9,
        "generated_text": "objective\n\n",
        "token_probs": [
          [
            "objective",
            0.11844566464424133
          ],
          [
            "\n\n",
            0.3142589032649994
          ]
        ],
        "inference_time": 1.1727409362792969
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 51,
    "sentence": "She now has her 26-year-old granddaughter, Eliza Brunero, lodging with her on Wednesdays each week.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.20214033126831055
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2153589725494385
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.7978596687316895
          ]
        ],
        "inference_time": 1.2167785167694092
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.7978596687316895
          ]
        ],
        "inference_time": 1.2220022678375244
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.7978596687316895
          ]
        ],
        "inference_time": 1.2150943279266357
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.7978596687316895
          ]
        ],
        "inference_time": 1.2305781841278076
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.7978596687316895
          ]
        ],
        "inference_time": 1.2310264110565186
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.7978596687316895
          ]
        ],
        "inference_time": 1.2265841960906982
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.20214033126831055
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2525115013122559
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.7978596687316895
          ]
        ],
        "inference_time": 1.2361130714416504
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.7978596687316895
          ]
        ],
        "inference_time": 1.2558963298797607
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 52,
    "sentence": "Wangari is one of 16 Kenyan women who have died allegedly at the hands of their partners since the start of 2024.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2146365642547607
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2267084121704102
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2550771236419678
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2142674922943115
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2594056129455566
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2193539142608643
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.255763053894043
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.24436616897583
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2524476051330566
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2331175804138184
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 53,
    "sentence": "And the Caesar Kunikov's watery demise is just the latest blow to have embarrassed Putin.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9426031112670898
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.266174077987671
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9426031112670898
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2315418720245361
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9426031112670898
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2604408264160156
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9426031112670898
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2260441780090332
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9426031112670898
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.253725290298462
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9426031112670898
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2786235809326172
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9426031112670898
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2636487483978271
      },
      {
        "repetition": 7,
        "generated_text": "```python",
        "token_probs": [
          [
            "```",
            0.057396817952394485
          ],
          [
            "python",
            1.0
          ]
        ],
        "inference_time": 1.2180767059326172
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9426031112670898
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2714271545410156
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9426031112670898
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2171740531921387
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 54,
    "sentence": "Beks then shared a clip of what happened straight after and continues: \"It was my lunch break and I just dipped out and didn't tell anybody.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9198377132415771
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2168807983398438
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9198377132415771
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2543511390686035
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9198377132415771
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2205991744995117
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9198377132415771
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2534949779510498
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9198377132415771
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2120864391326904
      },
      {
        "repetition": 5,
        "generated_text": "objective\n",
        "token_probs": [
          [
            "objective",
            0.08016232401132584
          ],
          [
            "\n",
            0.571223795413971
          ]
        ],
        "inference_time": 1.2666010856628418
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9198377132415771
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2287054061889648
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9198377132415771
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2798810005187988
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9198377132415771
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.213832139968872
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9198377132415771
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2568824291229248
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 55,
    "sentence": "I know you was heartbroken lol.\"",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8293257831239771
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.1269872188568115
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8293257831239771
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.1785626411437988
      },
      {
        "repetition": 2,
        "generated_text": "```sql",
        "token_probs": [
          [
            "```",
            0.05499953776597977
          ],
          [
            "sql",
            0.11129628866910934
          ]
        ],
        "inference_time": 1.135124683380127
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8293257831239771
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.152137041091919
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8293257831239771
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.1422204971313477
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8293257831239771
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.130566120147705
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.11567464807838412
          ]
        ],
        "inference_time": 1.133960485458374
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.11567464807838412
          ]
        ],
        "inference_time": 1.1274373531341553
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8293257831239771
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.1423611640930176
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8293257831239771
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.1719679832458496
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 56,
    "sentence": "“I can’t say I add anything to her life but she certainly brightens up mine.”  Brunero begs to differ.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.5912102460861206
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2400612831115723
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.5912102460861206
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2189695835113525
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.5912102460861206
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2166590690612793
      },
      {
        "repetition": 3,
        "generated_text": "“subject",
        "token_probs": [
          [
            "“",
            0.4087896943092346
          ],
          [
            "subject",
            0.8332225680351257
          ]
        ],
        "inference_time": 1.227557897567749
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.5912102460861206
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.215555191040039
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.5912102460861206
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2264783382415771
      },
      {
        "repetition": 6,
        "generated_text": "“subject",
        "token_probs": [
          [
            "“",
            0.4087896943092346
          ],
          [
            "subject",
            0.8332225680351257
          ]
        ],
        "inference_time": 1.2188315391540527
      },
      {
        "repetition": 7,
        "generated_text": "“subject",
        "token_probs": [
          [
            "“",
            0.4087896943092346
          ],
          [
            "subject",
            0.8332225680351257
          ]
        ],
        "inference_time": 1.211937665939331
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.5912102460861206
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2429063320159912
      },
      {
        "repetition": 9,
        "generated_text": "“subject",
        "token_probs": [
          [
            "“",
            0.4087896943092346
          ],
          [
            "subject",
            0.8332225680351257
          ]
        ],
        "inference_time": 1.2254891395568848
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 57,
    "sentence": "But while Leo Woodall is enjoying a career boost for his portrayal as 'pampered Southern toff' Dexter Mayhew, the 2011 film adaptation had the exact opposite effect for actor Jim Sturgess.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.16170717775821686
          ]
        ],
        "inference_time": 1.2670700550079346
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8382927775382996
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.243440866470337
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8382927775382996
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2698032855987549
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8382927775382996
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.229062557220459
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8382927775382996
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2746572494506836
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8382927775382996
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2365288734436035
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8382927775382996
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.239682912826538
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8382927775382996
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2597899436950684
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8382927775382996
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2289416790008545
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8382927775382996
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2703642845153809
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 58,
    "sentence": "I scribbled down what I saw and what I felt and the song kind of wrote itself.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2226951122283936
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.254164218902588
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2140929698944092
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.254012107849121
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.225039005279541
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.252776861190796
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2132809162139893
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2750458717346191
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2185635566711426
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.254828929901123
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 59,
    "sentence": "In the first image, the Emmy-nominated actress embraced Blanco, who has a passion for cooking, while he was preparing a platter of meatballs.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2303190231323242
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2750301361083984
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2173376083374023
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2564711570739746
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.228628158569336
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2477588653564453
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.246302604675293
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2171883583068848
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2748277187347412
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2263038158416748
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 60,
    "sentence": "It follows their range of Unmanned Surface Vehicles (USVs) which have been hugely successful in Black Sea attacks.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7350187301635742
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2638158798217773
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.2649812400341034
          ]
        ],
        "inference_time": 1.218550443649292
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7350187301635742
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2639870643615723
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7350187301635742
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2262403964996338
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7350187301635742
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2461283206939697
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.2649812400341034
          ]
        ],
        "inference_time": 1.221513271331787
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7350187301635742
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2327516078948975
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7350187301635742
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2226126194000244
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7350187301635742
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2300841808319092
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7350187301635742
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.216660499572754
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 61,
    "sentence": "But one person who knows exactly what's that like after getting the winning number on a scratchcard has told how they were left bitterly \"disappointed.\"",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.221182107925415
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2251930236816406
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2205333709716797
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2221660614013672
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2152099609375
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.211334228515625
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2169184684753418
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2209043502807617
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2401454448699951
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2350547313690186
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 62,
    "sentence": "Selena looked chic in a brown turtleneck, with her hair brushed back into a ponytail, while her other half rocked a white tee, and gold chains.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2777290344238281
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2263729572296143
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2266879081726074
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2606298923492432
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2261793613433838
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2621150016784668
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.210601568222046
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2559127807617188
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2215137481689453
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2483112812042236
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 63,
    "sentence": "The refreshing beer-and-fizzy-pop combination bubbled away as a quietly constant – if not cult – pub choice for over half my lifetime, then seemed to fizzle out over the past 15 years.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2312073707580566
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2704565525054932
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2219769954681396
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.317657232284546
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.226893424987793
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.265892505645752
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2383637428283691
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2658782005310059
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2275924682617188
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2735962867736816
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 64,
    "sentence": "Kyiv continues to target Russia’s Black Sea Fleet with great effectDr Bastian Giegerich, IISS security analyst  And Vlad's Black Sea fleet was \"put on the defensive by several events\", which included the sea drone attack in December that \"badly damaged a landing ship off Novorossiysk\".",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.40494258066426525
          ]
        ],
        "inference_time": 1.2532334327697754
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.40494258066426525
          ]
        ],
        "inference_time": 1.2816698551177979
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.4626989279149889
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.237990379333496
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.4626989279149889
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.273664951324463
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.4626989279149889
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2384979724884033
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.4626989279149889
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2572081089019775
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.40494258066426525
          ]
        ],
        "inference_time": 1.255739688873291
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.40494258066426525
          ]
        ],
        "inference_time": 1.2498431205749512
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.40494258066426525
          ]
        ],
        "inference_time": 1.2739429473876953
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.4626989279149889
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2398130893707275
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 65,
    "sentence": "“It’s such a bonus – sometimes I cook for her, sometimes she cooks for me, we play cards, we watch TV and we chat,” Hamilton says.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7730337381362915
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2607512474060059
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7730337381362915
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2191755771636963
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7730337381362915
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.259615421295166
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7730337381362915
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2186715602874756
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7730337381362915
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2480957508087158
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7730337381362915
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.224696397781372
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7730337381362915
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.219053030014038
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7730337381362915
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2188880443572998
      },
      {
        "repetition": 8,
        "generated_text": "“subject",
        "token_probs": [
          [
            "“",
            0.22696629166603088
          ],
          [
            "subject",
            1.0
          ]
        ],
        "inference_time": 1.21867036819458
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7730337381362915
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2126548290252686
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 66,
    "sentence": "She was fairly close to Wangari, and recalls with sadness how she had promised to send her money the following week so she could move to a new flat.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2298645973205566
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2274456024169922
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2161710262298584
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2141120433807373
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.215834140777588
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2124125957489014
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2259418964385986
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.208418369293213
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2476763725280762
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2274956703186035
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 67,
    "sentence": "And it could even be used to gather intelligence on Russian operations.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9486433863639832
          ]
        ],
        "inference_time": 1.2309234142303467
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9486433863639832
          ]
        ],
        "inference_time": 1.1911611557006836
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9486433863639832
          ]
        ],
        "inference_time": 1.226386308670044
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9486433863639832
          ]
        ],
        "inference_time": 1.1909675598144531
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9486433863639832
          ]
        ],
        "inference_time": 1.2069156169891357
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9486433863639832
          ]
        ],
        "inference_time": 1.2346258163452148
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9486433863639832
          ]
        ],
        "inference_time": 1.19285249710083
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9486433863639832
          ]
        ],
        "inference_time": 1.239544153213501
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9486433863639832
          ]
        ],
        "inference_time": 1.1961519718170166
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9486433863639832
          ]
        ],
        "inference_time": 1.222820520401001
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 68,
    "sentence": "\"I'm not really a gambler - it was a $10k (£7.9k) a week for life scratch card and I scratched it off.\"",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7594934105873108
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2198796272277832
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7594934105873108
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.255612850189209
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7594934105873108
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2202095985412598
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7594934105873108
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2605595588684082
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7594934105873108
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2135801315307617
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.2405065894126892
          ]
        ],
        "inference_time": 1.2524924278259277
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7594934105873108
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2178943157196045
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7594934105873108
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.258014440536499
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.2405065894126892
          ]
        ],
        "inference_time": 1.2166638374328613
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7594934105873108
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2659552097320557
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 69,
    "sentence": "When it was an Emma and Dex day I felt good about it.'",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8442041873931885
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.1884846687316895
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8442041873931885
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.1951539516448975
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8442041873931885
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2270662784576416
      },
      {
        "repetition": 3,
        "generated_text": "```python",
        "token_probs": [
          [
            "```",
            0.09383168816566467
          ],
          [
            "python",
            0.9072203040122986
          ]
        ],
        "inference_time": 1.1974897384643555
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8442041873931885
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.239612102508545
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8442041873931885
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.1911468505859375
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8442041873931885
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2373530864715576
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8442041873931885
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2018861770629883
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8442041873931885
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2350282669067383
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8442041873931885
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.1968331336975098
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 70,
    "sentence": "RULE 10: BE HAPPY  She then advised: “Like the song don’t worry be happy and create a life of meaningful memories.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.5429779245156752
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2703311443328857
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.5429779245156752
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.21525239944458
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.5429779245156752
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2592101097106934
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.5429779245156752
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2365975379943848
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.5429779245156752
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2371745109558105
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.5429779245156752
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.242318868637085
      },
      {
        "repetition": 6,
        "generated_text": "```python",
        "token_probs": [
          [
            "```",
            0.27351078391075134
          ],
          [
            "python",
            1.0
          ]
        ],
        "inference_time": 1.2167844772338867
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.11233614280717674
          ]
        ],
        "inference_time": 1.219287395477295
      },
      {
        "repetition": 8,
        "generated_text": "```python",
        "token_probs": [
          [
            "```",
            0.27351078391075134
          ],
          [
            "python",
            1.0
          ]
        ],
        "inference_time": 1.2231919765472412
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.5429779245156752
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2286407947540283
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 71,
    "sentence": "But perhaps most impressively, Ukrainian forces managed to blow up Russia's flagship vessel - the Moskva - in April 2022.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2119290828704834
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2272837162017822
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2099239826202393
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2157959938049316
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.212644338607788
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.214937448501587
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2071473598480225
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2302298545837402
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2136883735656738
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2449750900268555
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 72,
    "sentence": "But Michelle evidently didn't hold the blunder against him; the couple have been married since 2015 after meeting in late 2012.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2264072895050049
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2492127418518066
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.210839033126831
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2456700801849365
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2128922939300537
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2546300888061523
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.212831974029541
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2455885410308838
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2194247245788574
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2567718029022217
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 73,
    "sentence": "Vigils, dubbed “Dark Valentine”, were held across Kenya this week after a month in which more than a dozen women have been killed, allegedly by their partners.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.1989840418100357
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2140207290649414
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.8010159730911255
          ]
        ],
        "inference_time": 1.2578685283660889
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.8010159730911255
          ]
        ],
        "inference_time": 1.2303135395050049
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.8010159730911255
          ]
        ],
        "inference_time": 1.261441707611084
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.1989840418100357
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2179930210113525
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.8010159730911255
          ]
        ],
        "inference_time": 1.2548723220825195
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.8010159730911255
          ]
        ],
        "inference_time": 1.2117128372192383
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.8010159730911255
          ]
        ],
        "inference_time": 1.2365963459014893
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.8010159730911255
          ]
        ],
        "inference_time": 1.2394614219665527
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.8010159730911255
          ]
        ],
        "inference_time": 1.213221549987793
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 74,
    "sentence": "The radio presenter revealed the Fool Me Once star rumbled his porky pies the next day and fumed: 'Mark, you're a liar!'",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2543222904205322
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.220266342163086
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.245168924331665
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2213773727416992
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2561054229736328
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2129740715026855
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2533988952636719
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2173357009887695
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2430734634399414
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2276687622070312
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 75,
    "sentence": "The snap comes after Selena surprised fans on Monday with a very racy photo of Benny grabbing her cleavage  Gomez has been linked with Blanco, a musical artist, producer, and songwriter, since last year  In the first image from the post, the Emmy-nominated actress embraced Blanco, who has a passion for cooking, while he was preparing a platter of meatballs  Blanco was posed behind Gomez as they relaxed in the kitchen area of a home alongside friends including The Bear actor Matty Matheson  On Sunday, the couple was seen kissing one another while posed against a velvet red couch in a shot Gomez posted to Instagram  Gomez and Blanco have frequently shared their light-hearted and romantic moments via Instagram in the early days of their relationship.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8334078192710876
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2777166366577148
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.16659224033355713
          ]
        ],
        "inference_time": 1.2905957698822021
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8334078192710876
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.332371473312378
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.16659224033355713
          ]
        ],
        "inference_time": 1.295032262802124
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.16659224033355713
          ]
        ],
        "inference_time": 1.3159747123718262
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8334078192710876
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2954716682434082
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8334078192710876
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2956056594848633
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8334078192710876
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2942605018615723
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8334078192710876
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2921972274780273
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.16659224033355713
          ]
        ],
        "inference_time": 1.2966437339782715
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 76,
    "sentence": "November 17, 2023  Sir, your article ‘BBC crisis over £1.7bn pension bill for stars’ (4/11/2023) is incorrect and significantly inflates the BBC’s obligation to fund its defined benefit pension scheme shortfall and employer contribution rate.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.056662824004888535
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2829787731170654
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8098016318898686
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2323215007781982
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8098016318898686
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2480740547180176
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8098016318898686
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2277812957763672
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.056662824004888535
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2278623580932617
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8098016318898686
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.229163408279419
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8098016318898686
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2452030181884766
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8098016318898686
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2268526554107666
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.056662824004888535
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.253666639328003
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.1335355561254552
          ]
        ],
        "inference_time": 1.2515032291412354
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 77,
    "sentence": "The crime currently falls under homicide provisions, which rights groups say do not account for the unequal power relations between men and women that drive and characterise the killings.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8341088891029358
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.254784107208252
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.1658911257982254
          ]
        ],
        "inference_time": 1.2128374576568604
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8341088891029358
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2567622661590576
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8341088891029358
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2206897735595703
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8341088891029358
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2592401504516602
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8341088891029358
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2176458835601807
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8341088891029358
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2510035037994385
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8341088891029358
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.213653564453125
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.1658911257982254
          ]
        ],
        "inference_time": 1.2548046112060547
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8341088891029358
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.216594934463501
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 78,
    "sentence": "Früh Natur Radler  2.5%; The Real Ale Store, £1.85 for 500ml; Hop Burns & Black, £2.30  If grapefruit is not your bag, seek out this lemony-fresh delight from Früh.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.08550672153535022
          ]
        ],
        "inference_time": 1.292630672454834
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.1403695046901703
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2371978759765625
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.08550672153535022
          ]
        ],
        "inference_time": 1.2684311866760254
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.6338211776522087
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.233659029006958
      },
      {
        "repetition": 4,
        "generated_text": "```python",
        "token_probs": [
          [
            "```",
            0.07364378124475479
          ],
          [
            "python",
            0.9370970726013184
          ]
        ],
        "inference_time": 1.286330223083496
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.08550672153535022
          ]
        ],
        "inference_time": 1.2277686595916748
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.6338211776522087
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2306385040283203
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.6338211776522087
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2756600379943848
      },
      {
        "repetition": 8,
        "generated_text": "objective subjective",
        "token_probs": [
          [
            "objective",
            0.0666588842868805
          ],
          [
            " subjective",
            0.5889304280281067
          ]
        ],
        "inference_time": 1.2290043830871582
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.6338211776522087
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.267338752746582
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 79,
    "sentence": "Following on from this, Jim went on to star in the 2012 adaptation of David Mitchell's Cloud Atlas, which also divided audiences.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.2280607968568802
          ]
        ],
        "inference_time": 1.2114002704620361
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.771939218044281
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2552714347839355
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.771939218044281
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2219085693359375
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.771939218044281
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2493672370910645
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.771939218044281
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2239398956298828
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.771939218044281
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2513084411621094
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.2280607968568802
          ]
        ],
        "inference_time": 1.218458890914917
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.771939218044281
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2494404315948486
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.771939218044281
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.220027208328247
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.771939218044281
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2638862133026123
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 80,
    "sentence": "Limit your spending and you have limited your income too.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "```python",
        "token_probs": [
          [
            "```",
            0.12953980267047882
          ],
          [
            "python",
            1.0
          ]
        ],
        "inference_time": 1.198354721069336
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.2512346056892216
          ]
        ],
        "inference_time": 1.2241616249084473
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.10995493829250336
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2016117572784424
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.24080163286083778
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.1985015869140625
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.24080163286083778
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.1985423564910889
      },
      {
        "repetition": 5,
        "generated_text": "objective\n\n",
        "token_probs": [
          [
            "objective",
            0.22976583242416382
          ],
          [
            "\n\n",
            0.47931233048439026
          ]
        ],
        "inference_time": 1.2063214778900146
      },
      {
        "repetition": 6,
        "generated_text": "objective\n",
        "token_probs": [
          [
            "objective",
            0.22976583242416382
          ],
          [
            "\n",
            0.5206876993179321
          ]
        ],
        "inference_time": 1.1974048614501953
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.2512346056892216
          ]
        ],
        "inference_time": 1.1873321533203125
      },
      {
        "repetition": 8,
        "generated_text": "```python",
        "token_probs": [
          [
            "```",
            0.12953980267047882
          ],
          [
            "python",
            1.0
          ]
        ],
        "inference_time": 1.189466953277588
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.24080163286083778
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.192760705947876
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 81,
    "sentence": "Unlike some of the other lemon offerings out there, this isn’t soapy at all and has a very natural lemon flavouring.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2137742042541504
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2118449211120605
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2383463382720947
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2117207050323486
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2318034172058105
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2266104221343994
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2250962257385254
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2190325260162354
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2481684684753418
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2115652561187744
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 82,
    "sentence": "Blanco was past rumored to be in a romance with Elsie Hewitt, a 27-year-old model-actress from London (who has since been linked with Jason Sudeikis, 48).",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective\n\n",
        "token_probs": [
          [
            "objective",
            0.13049672544002533
          ],
          [
            "\n\n",
            0.6151002645492554
          ]
        ],
        "inference_time": 1.2594943046569824
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.7604325624692798
          ]
        ],
        "inference_time": 1.225682020187378
      },
      {
        "repetition": 2,
        "generated_text": "objective\n",
        "token_probs": [
          [
            "objective",
            0.13049672544002533
          ],
          [
            "\n",
            0.14557813107967377
          ]
        ],
        "inference_time": 1.2609734535217285
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.7604325624692798
          ]
        ],
        "inference_time": 1.2353014945983887
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.7604325624692798
          ]
        ],
        "inference_time": 1.2627496719360352
      },
      {
        "repetition": 5,
        "generated_text": "objective.",
        "token_probs": [
          [
            "objective",
            0.13049672544002533
          ],
          [
            ".",
            0.2393215447664261
          ]
        ],
        "inference_time": 1.2333920001983643
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.7604325624692798
          ]
        ],
        "inference_time": 1.281963586807251
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.7604325624692798
          ]
        ],
        "inference_time": 1.2364351749420166
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.7604325624692798
          ]
        ],
        "inference_time": 1.2641956806182861
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.7604325624692798
          ]
        ],
        "inference_time": 1.2269580364227295
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 83,
    "sentence": "The Schedule of Contributions agreed with the Scheme Trustee as part of the 2022 actuarial valuation states that the employer contribution rate for the defined benefit pension scheme is currently set to 30% of members’ pensionable salaries.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2777044773101807
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2298102378845215
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2685914039611816
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2341790199279785
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.229053258895874
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.269322156906128
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.232090950012207
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2554278373718262
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2308578491210938
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2764005661010742
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 84,
    "sentence": "So I wanted more.'",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.28892463045263383
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.1300175189971924
      },
      {
        "repetition": 1,
        "generated_text": "objective\n",
        "token_probs": [
          [
            "objective",
            0.03270283713936806
          ],
          [
            "\n",
            0.6428609490394592
          ]
        ],
        "inference_time": 1.166794776916504
      },
      {
        "repetition": 2,
        "generated_text": "```python",
        "token_probs": [
          [
            "```",
            0.23552018404006958
          ],
          [
            "python",
            0.9022914171218872
          ]
        ],
        "inference_time": 1.1355268955230713
      },
      {
        "repetition": 3,
        "generated_text": "Note:",
        "token_probs": [
          [
            "Note",
            0.27477389574050903
          ],
          [
            ":",
            1.0
          ]
        ],
        "inference_time": 1.1566195487976074
      },
      {
        "repetition": 4,
        "generated_text": "Note:",
        "token_probs": [
          [
            "Note",
            0.27477389574050903
          ],
          [
            ":",
            1.0
          ]
        ],
        "inference_time": 1.1196393966674805
      },
      {
        "repetition": 5,
        "generated_text": "```python",
        "token_probs": [
          [
            "```",
            0.23552018404006958
          ],
          [
            "python",
            0.9022914171218872
          ]
        ],
        "inference_time": 1.1259050369262695
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.28892463045263383
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.1590571403503418
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.28892463045263383
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.1376919746398926
      },
      {
        "repetition": 8,
        "generated_text": "objective\n",
        "token_probs": [
          [
            "objective",
            0.03270283713936806
          ],
          [
            "\n",
            0.6428609490394592
          ]
        ],
        "inference_time": 1.1576859951019287
      },
      {
        "repetition": 9,
        "generated_text": "Note:",
        "token_probs": [
          [
            "Note",
            0.27477389574050903
          ],
          [
            ":",
            1.0
          ]
        ],
        "inference_time": 1.1210777759552002
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 85,
    "sentence": "Nearly half of Republicans and right-leaning independents said the US was providing too much aid to Ukraine, according to a survey by the Pew Research Center conducted late last year.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2517385482788086
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.219559907913208
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2609367370605469
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2255854606628418
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.237997055053711
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2123363018035889
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2231557369232178
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2187986373901367
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2182507514953613
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2134740352630615
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 86,
    "sentence": "Meanwhile, some arch-conservatives suggested it was time for McConnell to step down.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7944874167442322
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2039055824279785
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7944874167442322
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2025878429412842
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7944874167442322
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.1909663677215576
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7944874167442322
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.189455270767212
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.20551253855228424
          ]
        ],
        "inference_time": 1.2009541988372803
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7944874167442322
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.196129560470581
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7944874167442322
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.1909441947937012
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7944874167442322
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.1906802654266357
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7944874167442322
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.1842308044433594
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7944874167442322
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2104368209838867
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 87,
    "sentence": "“I maintain a healthy weight that preserves my facial structure and collagen.”",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.16300447997629774
          ]
        ],
        "inference_time": 1.1898045539855957
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.5391934503059943
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2284905910491943
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.16300447997629774
          ]
        ],
        "inference_time": 1.2097060680389404
      },
      {
        "repetition": 3,
        "generated_text": "“subject",
        "token_probs": [
          [
            "“",
            0.2978020906448364
          ],
          [
            "subject",
            0.737448513507843
          ]
        ],
        "inference_time": 1.2422552108764648
      },
      {
        "repetition": 4,
        "generated_text": "“subject",
        "token_probs": [
          [
            "“",
            0.2978020906448364
          ],
          [
            "subject",
            0.737448513507843
          ]
        ],
        "inference_time": 1.2052068710327148
      },
      {
        "repetition": 5,
        "generated_text": "“subject",
        "token_probs": [
          [
            "“",
            0.2978020906448364
          ],
          [
            "subject",
            0.737448513507843
          ]
        ],
        "inference_time": 1.242326021194458
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.5391934503059943
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.1936864852905273
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.5391934503059943
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2299816608428955
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.5391934503059943
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.19571852684021
      },
      {
        "repetition": 9,
        "generated_text": "“subject",
        "token_probs": [
          [
            "“",
            0.2978020906448364
          ],
          [
            "subject",
            0.737448513507843
          ]
        ],
        "inference_time": 1.2404353618621826
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 88,
    "sentence": "Later in a campaign speech, Trump rattled American allies in Europe when he claimed that he would encourage Russia to attack Nato allies who did not pay enough to maintain the security alliance.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8201159834861755
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2139370441436768
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8201159834861755
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.237009048461914
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8201159834861755
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.227910041809082
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8201159834861755
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.219686508178711
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.17988398671150208
          ]
        ],
        "inference_time": 1.2520039081573486
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8201159834861755
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.219484567642212
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8201159834861755
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2499258518218994
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.17988398671150208
          ]
        ],
        "inference_time": 1.2059504985809326
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8201159834861755
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2632019519805908
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8201159834861755
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2268030643463135
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 89,
    "sentence": "Mark made the cheeky confession on-air while hosting his Heart FM radio programme on Monday, admitting he got 'clocked' by Michelle when she found the restaurant takeaway bag in the bin.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.6658660173416138
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.251216173171997
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.33413395285606384
          ]
        ],
        "inference_time": 1.2147412300109863
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.6658660173416138
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2616586685180664
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.33413395285606384
          ]
        ],
        "inference_time": 1.214522361755371
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.6658660173416138
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.255145788192749
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.6658660173416138
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2103240489959717
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.6658660173416138
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2725987434387207
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.6658660173416138
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2150096893310547
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.33413395285606384
          ]
        ],
        "inference_time": 1.2518057823181152
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.6658660173416138
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2171306610107422
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 90,
    "sentence": "“It has my facial expressions – my eyes, lips, hair, arms, legs, chest and butt, all dimples and textures fully integrated.”  Fabulous will pay for your exclusive stories.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "“subject",
        "token_probs": [
          [
            "“",
            0.2042110562324524
          ],
          [
            "subject",
            0.8433513045310974
          ]
        ],
        "inference_time": 1.2167425155639648
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7063813547658633
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2488205432891846
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7063813547658633
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2286961078643799
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7063813547658633
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.25008225440979
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.08940757121441534
          ]
        ],
        "inference_time": 1.230478286743164
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7063813547658633
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2376093864440918
      },
      {
        "repetition": 6,
        "generated_text": "“subject",
        "token_probs": [
          [
            "“",
            0.2042110562324524
          ],
          [
            "subject",
            0.8433513045310974
          ]
        ],
        "inference_time": 1.224933385848999
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7063813547658633
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2114737033843994
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7063813547658633
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2315027713775635
      },
      {
        "repetition": 9,
        "generated_text": "“subject",
        "token_probs": [
          [
            "“",
            0.2042110562324524
          ],
          [
            "subject",
            0.8433513045310974
          ]
        ],
        "inference_time": 1.2113733291625977
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 91,
    "sentence": "Since the 2022 valuation, funding has improved.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.1854844093322754
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.1938176155090332
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.199721097946167
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.1957647800445557
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.1912939548492432
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.195523977279663
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.1942322254180908
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.1918623447418213
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.190256118774414
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 1.2220242023468018
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 92,
    "sentence": "He said: 'At a very young age, the local theatre in my town went to local schools to cast a load of kids to be in a professional production of Wind In The Willows.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7549540225994953
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2273428440093994
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.2079046090539718
          ]
        ],
        "inference_time": 1.2639491558074951
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7549540225994953
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.207648515701294
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7549540225994953
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2554450035095215
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7549540225994953
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2373461723327637
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7549540225994953
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2197740077972412
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7549540225994953
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2537553310394287
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7549540225994953
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2227931022644043
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.2079046090539718
          ]
        ],
        "inference_time": 1.2516937255859375
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7549540225994953
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2103118896484375
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 93,
    "sentence": "RULE 9: LOVE  According to Gina, you should love yourself, love your life, love others.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.6520595072618462
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2434101104736328
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.6520595072618462
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2074058055877686
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.6520595072618462
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2549757957458496
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.6520595072618462
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.212193489074707
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.6520595072618462
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2603161334991455
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.146087837272173
          ]
        ],
        "inference_time": 1.2295970916748047
      },
      {
        "repetition": 6,
        "generated_text": "```python",
        "token_probs": [
          [
            "```",
            0.17464083433151245
          ],
          [
            "python",
            1.0
          ]
        ],
        "inference_time": 1.2494921684265137
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.6520595072618462
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2121436595916748
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.6520595072618462
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2527611255645752
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.6520595072618462
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2116906642913818
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 94,
    "sentence": "Anne Hathaway is not it.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.193145047041412
          ]
        ],
        "inference_time": 1.165759801864624
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.193145047041412
          ]
        ],
        "inference_time": 1.1255886554718018
      },
      {
        "repetition": 2,
        "generated_text": "```python",
        "token_probs": [
          [
            "```",
            0.1478375345468521
          ],
          [
            "python",
            1.0
          ]
        ],
        "inference_time": 1.1288039684295654
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.4427935973677677
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.1713480949401855
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.193145047041412
          ]
        ],
        "inference_time": 1.1324186325073242
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.4427935973677677
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.1631555557250977
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.4427935973677677
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.1300020217895508
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.4427935973677677
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.1605219841003418
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.4427935973677677
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.1385831832885742
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.4427935973677677
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.1470551490783691
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 95,
    "sentence": "As she watches her granddaughters play, Wairimu wonders if things could have played out differently.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2389211654663086
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2109744548797607
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2393155097961426
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2115187644958496
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2473914623260498
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2227272987365723
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2334492206573486
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2214505672454834
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2078704833984375
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2215700149536133
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 96,
    "sentence": "RULE 5: ATTITUDE IS EVERYTHING  Gina stressed the importance of \"thinking yourself young\" as she suggested: \"You are what you think.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.4134468770089832
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2191736698150635
      },
      {
        "repetition": 1,
        "generated_text": "Objective\n",
        "token_probs": [
          [
            "Objective",
            0.10349448770284653
          ],
          [
            "\n",
            0.3745630085468292
          ]
        ],
        "inference_time": 1.2199382781982422
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.4134468770089832
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2152671813964844
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.07610600099007092
          ]
        ],
        "inference_time": 1.2180454730987549
      },
      {
        "repetition": 4,
        "generated_text": "```python",
        "token_probs": [
          [
            "```",
            0.23706983029842377
          ],
          [
            "python",
            0.9197113513946533
          ]
        ],
        "inference_time": 1.2231309413909912
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.4134468770089832
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.216099500656128
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.4134468770089832
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2167234420776367
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.4134468770089832
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.21543288230896
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.4134468770089832
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2166483402252197
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.4134468770089832
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2208359241485596
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 97,
    "sentence": "However, Variety wasn't so gushing - and said the drama 'should have stayed in quarantine'.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.6730038523674011
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.242602825164795
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.6730038523674011
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2188441753387451
      },
      {
        "repetition": 2,
        "generated_text": "objective\n\n",
        "token_probs": [
          [
            "objective",
            0.11826276779174805
          ],
          [
            "\n\n",
            0.6124098300933838
          ]
        ],
        "inference_time": 1.2577927112579346
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.6730038523674011
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2241175174713135
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.6730038523674011
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2598357200622559
      },
      {
        "repetition": 5,
        "generated_text": "```python",
        "token_probs": [
          [
            "```",
            0.1704748570919037
          ],
          [
            "python",
            1.0
          ]
        ],
        "inference_time": 1.209028959274292
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.6730038523674011
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2583794593811035
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.6730038523674011
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2302494049072266
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.6730038523674011
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2520747184753418
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.6730038523674011
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2176759243011475
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 98,
    "sentence": "The Ohio senator JD Vance, another Trump loyalist, claimed the effort to replenish Ukraine’s war chest was a “plot” by the Republican establishment to “stop the election of Donald Trump”.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2238450050354004
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2622485160827637
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.21382474899292
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2436864376068115
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.211902141571045
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2666831016540527
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2140252590179443
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2521333694458008
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.212352991104126
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2525577545166016
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 99,
    "sentence": "It’s made me realise how much we take older people for granted.”  Both Brunero and Hamilton can see how mutually beneficial a multigenerational community such as Ayrton House could be.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8039205074310303
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2074313163757324
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8039205074310303
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.251894474029541
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8039205074310303
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2190220355987549
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.19607950747013092
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2541956901550293
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8039205074310303
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2159390449523926
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8039205074310303
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2500824928283691
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8039205074310303
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2144577503204346
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.19607950747013092
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.252817153930664
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8039205074310303
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2072985172271729
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8039205074310303
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2264354228973389
      }
    ],
    "predicted_label": "subjective"
  }
]