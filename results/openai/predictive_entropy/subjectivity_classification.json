[
  {
    "sample_idx": 0,
    "sentence": "Blanco established himself earlier in his career working for Dr. Luke's Kasz Money Productions.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999998063873693
          ]
        ],
        "inference_time": 1.486854076385498
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999998063873693
          ]
        ],
        "inference_time": 0.6658737659454346
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999998063873693
          ]
        ],
        "inference_time": 0.876702070236206
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999998063873693
          ]
        ],
        "inference_time": 0.3159677982330322
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999998063873693
          ]
        ],
        "inference_time": 0.6727783679962158
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999998063873693
          ]
        ],
        "inference_time": 0.3190491199493408
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.6306376457214355
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999998063873693
          ]
        ],
        "inference_time": 0.7001781463623047
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999998063873693
          ]
        ],
        "inference_time": 0.3440420627593994
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999998063873693
          ]
        ],
        "inference_time": 0.4965324401855469
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 1,
    "sentence": "RULE 13: ARTIFICIAL INTELLIGENCE  Not only this, but Gina also created an AI model of herself to achieve immortality.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9988297481910151
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.40436482429504395
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9968256700650793
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.6132729053497314
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9988295101677532
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4104197025299072
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9990881180345619
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.41066479682922363
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9992896085614399
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3376033306121826
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9990877607328715
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.378645658493042
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9990881180345619
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4110872745513916
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9975261026243005
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3033609390258789
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9984979347814182
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5117461681365967
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9968261453887124
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4053018093109131
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 2,
    "sentence": "The valuation is required by law and the figure is assessed independently by a pension specialist and has been reviewed by the National Audit Office.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.403594970703125
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.606492280960083
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.32212162017822266
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999995679801056
          ]
        ],
        "inference_time": 0.40331435203552246
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.39903807640075684
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.42315196990966797
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 1.0171518325805664
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.36081504821777344
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999995679801056
          ]
        ],
        "inference_time": 0.31948161125183105
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.3016543388366699
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 3,
    "sentence": "A sip can really hit the spot after a long bike ride or a walk.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3129546642303467
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.38713502883911133
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.34749746322631836
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.46993088722229004
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.41594552993774414
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3188192844390869
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.1821329593658447
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4868755340576172
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4435715675354004
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4015507698059082
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 4,
    "sentence": "Lobster!\"",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9626532270199226
          ]
        ],
        "inference_time": 0.5287957191467285
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9241144846788162
          ]
        ],
        "inference_time": 0.3727233409881592
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9819972187875918
          ]
        ],
        "inference_time": 0.3445758819580078
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.4999845020802217
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.2966790199279785
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9626511219447033
          ]
        ],
        "inference_time": 0.2977116107940674
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9706706276519319
          ]
        ],
        "inference_time": 0.36529111862182617
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9770062309678087
          ]
        ],
        "inference_time": 0.3571443557739258
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9770062309678087
          ]
        ],
        "inference_time": 0.5727910995483398
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.37752939406939395
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5058584213256836
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9706312010709541
          ]
        ],
        "inference_time": 0.7212200164794922
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 5,
    "sentence": "But this is precisely the reason why Labour must reject the austerian urges that, inevitably, spring from the credit card analogy.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.506990909576416
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.39000678062438965
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4242384433746338
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.358994722366333
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4040360450744629
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5698482990264893
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2485828399658203
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3629646301269531
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5107376575469971
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.544140100479126
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 6,
    "sentence": "Googled how to cook a good lobster and I read how hard it is to get it good because it can turn very tough, rubbery.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999995679801056
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5032267570495605
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999995679801056
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.6106507778167725
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999995679801056
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5399365425109863
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999995679801056
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3877425193786621
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.999999448776502
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.40474963188171387
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999995679801056
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.38321828842163086
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999995679801056
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.34676313400268555
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999995679801056
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5644078254699707
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.999999448776502
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3375551700592041
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999995679801056
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5659215450286865
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 7,
    "sentence": "Apartments cost from \u00a3392 per week to rent, which makes it more expensive than an average room in first-year halls (a single room without bathroom typically costs \u00a3250 per week) but more affordable than a lot of the postgraduate accommodation on offer.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.36685705184936523
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.7591638565063477
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.35281896591186523
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999998063873693
          ]
        ],
        "inference_time": 0.3450009822845459
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.28649330139160156
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.35178637504577637
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999998063873693
          ]
        ],
        "inference_time": 0.49037885665893555
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999995679801056
          ]
        ],
        "inference_time": 0.3950834274291992
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999998063873693
          ]
        ],
        "inference_time": 0.46181344985961914
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999995679801056
          ]
        ],
        "inference_time": 0.3971123695373535
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 8,
    "sentence": "We apologise to TikTok for not approaching it for comment prior to publication.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999091165777
          ]
        ],
        "inference_time": 0.5493760108947754
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999989719621736
          ]
        ],
        "inference_time": 0.35550880432128906
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999980183344259
          ]
        ],
        "inference_time": 0.5605359077453613
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999980183344259
          ]
        ],
        "inference_time": 0.5680420398712158
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999980183344259
          ]
        ],
        "inference_time": 0.5580005645751953
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999980183344259
          ]
        ],
        "inference_time": 0.5139057636260986
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999992103693378
          ]
        ],
        "inference_time": 0.5075869560241699
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999997779927489
          ]
        ],
        "inference_time": 0.4117445945739746
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999980183344259
          ]
        ],
        "inference_time": 0.7113614082336426
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999987335551229
          ]
        ],
        "inference_time": 0.3052833080291748
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 9,
    "sentence": "Crumbling parliament patched up with a few fig leaves  Parliament gained some new residents yesterday, and there are already questions as to their expenses.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9706757895482996
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3714272975921631
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9706765760392978
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.33116674423217773
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9706765760392978
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.30670690536499023
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9398970437539441
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.41247129440307617
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9859295983907012
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3524799346923828
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9241279246663526
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 2.712198257446289
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9706757895482996
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3786303997039795
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9890078547938236
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4029886722564697
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9859295983907012
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.45372724533081055
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9859295983907012
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.573284387588501
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 10,
    "sentence": "In 2019, he bagged a role in the ensemble movie Berlin, I Love You - which was labelled a 'empty, boring flop' by The Observer.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9398865116338233
          ]
        ],
        "inference_time": 0.33954358100891113
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.8519167451484474
          ]
        ],
        "inference_time": 1.129302978515625
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.8519286019394828
          ]
        ],
        "inference_time": 0.4617781639099121
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.8519286019394828
          ]
        ],
        "inference_time": 0.318636417388916
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.7772613203035215
          ]
        ],
        "inference_time": 0.32007765769958496
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9525484773357404
          ]
        ],
        "inference_time": 0.30434703826904297
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.8519286019394828
          ]
        ],
        "inference_time": 0.531848669052124
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.5621498975836102
          ]
        ],
        "inference_time": 0.4104745388031006
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.8519286019394828
          ]
        ],
        "inference_time": 0.6078855991363525
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9706559105219225
          ]
        ],
        "inference_time": 0.31302380561828613
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 11,
    "sentence": "By the way, she is honestly the best cook.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3881216049194336
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4169344902038574
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5165832042694092
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.469118595123291
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3484189510345459
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4084484577178955
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4051969051361084
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4175558090209961
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3374505043029785
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4906654357910156
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 12,
    "sentence": "Families are doubling down on calls for perpetrators to be brought to justice, and say changes on the handling of femicide cases are necessary.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9046454507913579
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.30042457580566406
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.998498172958146
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.38005971908569336
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9933060681422146
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.46677470207214355
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9046454507913579
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4392995834350586
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9980726312219192
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3419513702392578
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9975263404530849
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.41381072998046875
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9890118154687233
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4140901565551758
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9994468818751125
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3259584903717041
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9933061870160765
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4764537811279297
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9890115870386701
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4551243782043457
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 13,
    "sentence": "Anything we can actually do, we can afford.\u201d Britain\u2019s conundrum, today, is that the next government, whose job will be to fix the Tories\u2019 mess, is led by politicians who share neither Keynes\u2019s aims nor his innovative approach to public finance.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999998063873693
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3062896728515625
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.40332818031311035
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999996871837232
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5634937286376953
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999996871837232
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.330547571182251
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.5323941707611084
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999998063873693
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.639564037322998
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.30587148666381836
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4145321846008301
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999996871837232
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.6167030334472656
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999998063873693
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.6085741519927979
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 14,
    "sentence": "\u201cI just believe in being the best version of myself that I can possibly be, it makes me feel good.\u201d  Read more real life stories  Not only does Gina swear by hydration - but she also has 13 other rules she sticks to like glue.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.41103625297546387
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.41283130645751953
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5103366374969482
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.41071128845214844
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3178534507751465
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.39428281784057617
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4734170436859131
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.43766117095947266
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.7409205436706543
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.38899993896484375
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 15,
    "sentence": "\u201cRussia\u2019s dominance in the Black Sea is now challenged.\u201d  The Boxing Day blast saw Vlad's valuable landing ship - docked in Crimea - turned into a raging fireball.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9046228244450095
          ]
        ],
        "inference_time": 0.3366813659667969
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.6791585568941607
          ]
        ],
        "inference_time": 0.4081752300262451
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.8175447784549873
          ]
        ],
        "inference_time": 0.5095014572143555
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9398728284572221
          ]
        ],
        "inference_time": 0.6120693683624268
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9398728284572221
          ]
        ],
        "inference_time": 0.4002406597137451
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9046026317344515
          ]
        ],
        "inference_time": 0.3351736068725586
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.8175294897279534
          ]
        ],
        "inference_time": 0.3255653381347656
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.8175447784549873
          ]
        ],
        "inference_time": 0.46169614791870117
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.8175294897279534
          ]
        ],
        "inference_time": 0.36968469619750977
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9525375515065483
          ]
        ],
        "inference_time": 0.30258798599243164
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 16,
    "sentence": "Sea drones & anti-ship missiles  The February 1 attack came after Ukraine unveiled its new underwater robot drone, a stealth Autonomous Underwater Vehicle (AUV).",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.3371098041534424
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.4233672618865967
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            1.0
          ]
        ],
        "inference_time": 0.4040489196777344
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.37610459327697754
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.6427009105682373
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999998063873693
          ]
        ],
        "inference_time": 0.3644897937774658
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.3507850170135498
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.6123201847076416
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.47876644134521484
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999998063873693
          ]
        ],
        "inference_time": 0.45755815505981445
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 17,
    "sentence": "\u201cThey\u2019re people who appreciate the onsite facilities such as the restaurant and gym and see living around older people as a lifestyle benefit rather than a hindrance,\u201d she explains.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.999999448776502
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.695930004119873
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.999999448776502
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.41074109077453613
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.999999448776502
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.554445743560791
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999998063873693
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.37195515632629395
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999987335551229
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5396709442138672
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.999999448776502
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.38064074516296387
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.999999448776502
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5088512897491455
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999995679801056
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5088551044464111
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999989719621736
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.47310304641723633
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.999999448776502
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4006235599517822
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 18,
    "sentence": "But none of this means that the ditched \u00a328bn policy was optimal or, indeed, that an incoming chancellor can safely commit the Treasury to borrow and spend unlimited amounts.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9933046513703944
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.45699167251586914
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9933046513703944
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4855005741119385
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.904641750465446
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5316407680511475
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9770177249927501
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3487882614135742
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.09534852763891467
          ]
        ],
        "inference_time": 0.37552404403686523
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8519410555561465
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.37761425971984863
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8807898675929635
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.43996739387512207
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9626664815686116
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4098522663116455
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.5621614575294767
          ]
        ],
        "inference_time": 0.3872697353363037
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9980720363244501
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3568382263183594
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 19,
    "sentence": "The record, which topped out at 41 on the Billboard 200, included contributions from Bieber, Halsey, Calvin Harris, Omar Apollo and Gracie Abrams.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.37685680389404297
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.41062331199645996
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.40433311462402344
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.5153379440307617
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.41057372093200684
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.37279367446899414
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.5425395965576172
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999448776502
          ]
        ],
        "inference_time": 0.4120035171508789
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999995679801056
          ]
        ],
        "inference_time": 0.512507438659668
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.31076955795288086
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 20,
    "sentence": "The journalist Mike Smith was struck yesterday when he noticed",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9770001964768644
          ]
        ],
        "inference_time": 0.5705661773681641
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9968105078707143
          ]
        ],
        "inference_time": 0.32805776596069336
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9819812452793024
          ]
        ],
        "inference_time": 0.5124862194061279
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9975039144236071
          ]
        ],
        "inference_time": 1.0231704711914062
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9947577358176573
          ]
        ],
        "inference_time": 1.1597590446472168
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.97699723929874
          ]
        ],
        "inference_time": 0.30698370933532715
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9889874474375415
          ]
        ],
        "inference_time": 0.5835990905761719
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9769906370896643
          ]
        ],
        "inference_time": 0.3994584083557129
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9889874474375415
          ]
        ],
        "inference_time": 0.4167656898498535
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9889874474375415
          ]
        ],
        "inference_time": 0.5148224830627441
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 21,
    "sentence": "Two years later, he stepped into a leading man role once again when he appeared in The Other Me - about a architect who has an eye disease which enables him to see people's real motives.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999448776502
          ]
        ],
        "inference_time": 0.5025041103363037
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999995679801056
          ]
        ],
        "inference_time": 0.32109880447387695
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999448776502
          ]
        ],
        "inference_time": 0.4963078498840332
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.48058652877807617
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.3430819511413574
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.3802826404571533
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999995679801056
          ]
        ],
        "inference_time": 0.7402288913726807
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999995679801056
          ]
        ],
        "inference_time": 0.4075620174407959
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999448776502
          ]
        ],
        "inference_time": 0.6271286010742188
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.39777588844299316
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 22,
    "sentence": "At the time, Time magazine dubbed the film 'disjoined' - saying that 'characters that Nicholls brought so cunningly to life in the book feel rushed through a timeline, tied to an agenda'.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9914198484694933
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.2858884334564209
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9046423435949925
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3846244812011719
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9626672561907378
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.34834885597229004
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.6224434320986044
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.31177783012390137
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9770184056121471
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5069231986999512
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.970681293190635
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5043880939483643
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9626668115001817
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.41324949264526367
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.8807856151706873
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5110728740692139
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7772858979002809
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.328904151916504
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.977017381043556
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.40538668632507324
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 23,
    "sentence": "\u201cWe were in shock,\u201d says Wangari\u2019s father.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999992103693378
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5093348026275635
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999996871837232
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.6135709285736084
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999998063873693
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.511326789855957
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999996871837232
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4115314483642578
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.46260762214660645
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.999999448776502
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.35051774978637695
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999996871837232
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4113330841064453
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999992103693378
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5109531879425049
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999996871837232
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.30702781677246094
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999996871837232
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.402909517288208
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 24,
    "sentence": "House Democrats and the remaining pro-Ukraine House Republicans are casting about behind the scenes for a solution.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999448776502
          ]
        ],
        "inference_time": 0.3746297359466553
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999995679801056
          ]
        ],
        "inference_time": 0.5505363941192627
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999448776502
          ]
        ],
        "inference_time": 0.40758681297302246
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999448776502
          ]
        ],
        "inference_time": 0.5110957622528076
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999448776502
          ]
        ],
        "inference_time": 0.4115757942199707
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999448776502
          ]
        ],
        "inference_time": 0.40167903900146484
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999448776502
          ]
        ],
        "inference_time": 0.40816664695739746
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999448776502
          ]
        ],
        "inference_time": 0.4105985164642334
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999448776502
          ]
        ],
        "inference_time": 0.5112273693084717
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.40987062454223633
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 25,
    "sentence": "Austerity, and the credit card analogy that provides its thin veneer of logic, is not just bad for workers and people in desperate need of state support during tough times; it also depresses investment.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.715308427810669
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4075145721435547
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.0284991264343262
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.6091868877410889
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.6196804046630859
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5105814933776855
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.35942912101745605
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.46030378341674805
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.402423620223999
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4303150177001953
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 26,
    "sentence": "In a pre-dawn vote on Tuesday, Graham joined the majority of Senate Republicans in opposing a foreign aid package that would rush wartime assistance to Ukraine as it approaches the second anniversary of Russia\u2019s full invasion.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.4918060302734375
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999998063873693
          ]
        ],
        "inference_time": 0.40685081481933594
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.4143972396850586
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.40502429008483887
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.5150139331817627
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.3085157871246338
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.6136646270751953
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.32364702224731445
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.3171520233154297
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.37546586990356445
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 27,
    "sentence": "The Observer's Philip French dubbed it 'thin, superficial and sentimental' and said the casting of Anne Hathway was 'disastrous'.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3012821674346924
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4601461887359619
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.46320509910583496
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.38126611709594727
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.30558156967163086
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.43091630935668945
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.48648738861083984
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.9463741779327393
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5305147171020508
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3918614387512207
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 28,
    "sentence": "A third commented: \"$20 for $6k - not bad!\"",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999995679801056
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.39087796211242676
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999995679801056
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4167306423187256
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.999999448776502
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5232124328613281
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999995679801056
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.33535027503967285
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999995679801056
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.29470300674438477
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999998063873693
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3931705951690674
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999993295729128
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4546785354614258
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999995679801056
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5739223957061768
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4055318832397461
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999998063873693
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.40795040130615234
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 29,
    "sentence": "The plan incorporates cash payments supplemented by contingent contributions.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.40407752990722656
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.3383188247680664
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.5598795413970947
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.3251228332519531
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.5765621662139893
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.37688708305358887
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.3784019947052002
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.4081449508666992
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 3.614752769470215
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.34753942489624023
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 30,
    "sentence": "It is true that the Tories will leave scorched earth behind for the next government, with a budget dripping in red ink and a pitiful level of investment in the technologies and services the UK needs to escape a long-term slump.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4248545169830322
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5105125904083252
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.38266420364379883
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.47068262100219727
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5281336307525635
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.357738733291626
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.781829833984375
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5672969818115234
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3862271308898926
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.9166877269744873
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 31,
    "sentence": "A WOMAN who has been dubbed \u2018the world\u2019s hottest gran\u2019 has revealed how she stays looking eternally young at the age of 53.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.49999195248539136
          ]
        ],
        "inference_time": 0.38629794120788574
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.6791652565334548
          ]
        ],
        "inference_time": 1.629194736480713
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9859241542882929
          ]
        ],
        "inference_time": 0.3349483013153076
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9819990607054618
          ]
        ],
        "inference_time": 0.32227492332458496
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9820002130490838
          ]
        ],
        "inference_time": 0.3114774227142334
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.8519356094567364
          ]
        ],
        "inference_time": 13.379289150238037
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7310531534529919
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.31976938247680664
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.49999281673957713
          ]
        ],
        "inference_time": 0.35063934326171875
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9820002130490838
          ]
        ],
        "inference_time": 0.37021970748901367
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.5621673883765769
          ]
        ],
        "inference_time": 0.37224769592285156
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 32,
    "sentence": "Couldn't say everything I wanted to in the video.\"",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.999999448776502
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5437774658203125
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999992103693378
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.33730459213256836
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.999999448776502
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.29805946350097656
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999996871837232
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.38339710235595703
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999996871837232
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4074842929840088
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999995679801056
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5120749473571777
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.999999448776502
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.40802812576293945
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999995679801056
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.6189396381378174
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.999999448776502
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4099240303039551
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999992103693378
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.44635772705078125
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 33,
    "sentence": "From the Senate floor, Senator Mitch McConnell, the top Republican, delivered increasingly urgent pleas for his conference to rise to the occasion and support America\u2019s allies, even after his plan to tie border security to foreign aid collapsed, torpedoed by Trump\u2019s opposition.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999995679801056
          ]
        ],
        "inference_time": 0.4709663391113281
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999448776502
          ]
        ],
        "inference_time": 0.40727901458740234
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999995679801056
          ]
        ],
        "inference_time": 0.4404582977294922
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999995679801056
          ]
        ],
        "inference_time": 0.4976377487182617
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999995679801056
          ]
        ],
        "inference_time": 0.3446364402770996
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999995679801056
          ]
        ],
        "inference_time": 0.4278292655944824
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999448776502
          ]
        ],
        "inference_time": 0.27538061141967773
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999995679801056
          ]
        ],
        "inference_time": 0.3643937110900879
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999448776502
          ]
        ],
        "inference_time": 0.7208559513092041
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999995679801056
          ]
        ],
        "inference_time": 0.33499813079833984
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 34,
    "sentence": "The Hollywood Reporter was particularly gushing of Jim Sturgess' performance - saying the actor had 'staked his claim as the new Hugh Grant only without the fussy mannerisms'.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999996871837232
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.47009897232055664
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.40561699867248535
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5000631809234619
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.520057201385498
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.46576809883117676
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999996871837232
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.45244765281677246
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3223135471343994
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3858780860900879
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5109257698059082
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999995679801056
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.8191003799438477
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 35,
    "sentence": "The Neptune \"super missile\", revealed in August last year, was reportedly snatched from behind enemy lines during a raid on Putin's prized \u00a3200million air defence system.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999091165777
          ]
        ],
        "inference_time": 0.3038620948791504
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999091165777
          ]
        ],
        "inference_time": 0.31467747688293457
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999091165777
          ]
        ],
        "inference_time": 0.40418124198913574
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999992103693378
          ]
        ],
        "inference_time": 1.2294886112213135
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999992103693378
          ]
        ],
        "inference_time": 0.40448904037475586
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999993295729128
          ]
        ],
        "inference_time": 0.29344654083251953
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999091165777
          ]
        ],
        "inference_time": 0.3685603141784668
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999091165777
          ]
        ],
        "inference_time": 0.8681674003601074
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999993295729128
          ]
        ],
        "inference_time": 0.5129015445709229
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999993295729128
          ]
        ],
        "inference_time": 0.4124128818511963
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 36,
    "sentence": "- Actor was on track to become a Hollywood star when he appeared in One Day  - Read More: How Taylor Swift's savvy marketing executive mother Andrea turned her daughter into a superstar  David Nicholls' romantic novel One Day became an overnight fan favourite when it was first released in 2009 and is now being rediscovered by a new audience thanks to Netflix's adaptation.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999491461219632
          ]
        ],
        "inference_time": 0.509589433670044
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9988228462371191
          ]
        ],
        "inference_time": 0.4125959873199463
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999452126087266
          ]
        ],
        "inference_time": 0.3179891109466553
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999738203321572
          ]
        ],
        "inference_time": 0.6021912097930908
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999909215726114
          ]
        ],
        "inference_time": 0.4047577381134033
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999757275417279
          ]
        ],
        "inference_time": 0.5117249488830566
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9996575634582938
          ]
        ],
        "inference_time": 0.41393423080444336
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999988527586979
          ]
        ],
        "inference_time": 0.6152222156524658
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999988527586979
          ]
        ],
        "inference_time": 0.6153218746185303
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999847868414117
          ]
        ],
        "inference_time": 0.40406084060668945
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 37,
    "sentence": "\"$6k is a lot of money,\" wrote one.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.46074676513671875
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3780066967010498
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3125433921813965
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5825483798980713
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4418370723724365
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5059080123901367
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5468926429748535
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.37178802490234375
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.410515308380127
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4329662322998047
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 38,
    "sentence": "He told The Independent in 2021: 'If someone does a bad one, I can\u2019t watch the film.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999828796127436
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5000660419464111
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999829988145424
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.49748754501342773
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.99999503825305
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.42851972579956055
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999919389781311
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4096965789794922
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999541524335852
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5112831592559814
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999541524335852
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4079601764678955
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.99999503825305
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.374859094619751
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999829988145424
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3406496047973633
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999244725260583
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.40717506408691406
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999829988145424
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5135040283203125
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 39,
    "sentence": "Gina Stewart has previously hit the headlines for her youthful appearance and most recently, immortalising herself as an AI model.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999659531252162
          ]
        ],
        "inference_time": 0.30491089820861816
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999375840250673
          ]
        ],
        "inference_time": 0.3019130229949951
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999815683975317
          ]
        ],
        "inference_time": 0.4100930690765381
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9998999187254098
          ]
        ],
        "inference_time": 0.4078338146209717
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999375840250673
          ]
        ],
        "inference_time": 0.6888041496276855
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999375840250673
          ]
        ],
        "inference_time": 0.41467738151550293
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999599931651877
          ]
        ],
        "inference_time": 0.47005677223205566
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999831180163555
          ]
        ],
        "inference_time": 0.49651241302490234
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999831180163555
          ]
        ],
        "inference_time": 0.5924439430236816
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999500997045827
          ]
        ],
        "inference_time": 0.41170382499694824
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 40,
    "sentence": "Anne is a very warm actress.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.47231507301330566
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.46745848655700684
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4563162326812744
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.6128711700439453
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.6191351413726807
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5052580833435059
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.44302916526794434
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.6712467670440674
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.7152941226959229
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5127651691436768
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 41,
    "sentence": "And I was good at it.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4032440185546875
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.41335630416870117
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5074307918548584
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.5358083248138428
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 2.40371036529541
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.468768835067749
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.6126782894134521
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.43775033950805664
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4828672409057617
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5084285736083984
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 42,
    "sentence": "\u201cOn matters like femicide which society takes lightly, you don\u2019t just get justice,\u201d says Kamande.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.40601110458374023
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5040004253387451
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.41196274757385254
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4128382205963135
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.41034436225891113
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3049907684326172
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5227186679840088
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.6100783348083496
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3278377056121826
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.37965869903564453
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 43,
    "sentence": "hosted by Laura Rangeley and Michael Deakin).",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999987335551229
          ]
        ],
        "inference_time": 0.5096135139465332
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999987335551229
          ]
        ],
        "inference_time": 0.6080842018127441
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999986143516758
          ]
        ],
        "inference_time": 0.6083710193634033
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999987335551229
          ]
        ],
        "inference_time": 0.5180606842041016
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999986143516758
          ]
        ],
        "inference_time": 0.409273624420166
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999983759447105
          ]
        ],
        "inference_time": 1.0603225231170654
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999986143516758
          ]
        ],
        "inference_time": 0.3697516918182373
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999984951481292
          ]
        ],
        "inference_time": 1.2287507057189941
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999986143516758
          ]
        ],
        "inference_time": 1.1238853931427002
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999984951481292
          ]
        ],
        "inference_time": 0.30629801750183105
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 44,
    "sentence": "Following One Day, the actor was dubbed the 'new Hugh Grant'  Pictured: Jim Sturgess - who is now a musician - opposite David Jason in A Touch of Frost in March 2003  Speaking to The Telegraph at the time, Jim - who starred in the Beatles-inspired movie Across the Universe beforehand - admitted that he hadn't read the book when he had his first audition.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999448776502
          ]
        ],
        "inference_time": 0.4086446762084961
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999448776502
          ]
        ],
        "inference_time": 0.4092888832092285
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999448776502
          ]
        ],
        "inference_time": 0.3961350917816162
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999993295729128
          ]
        ],
        "inference_time": 0.42169952392578125
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.511232852935791
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.4083292484283447
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999993295729128
          ]
        ],
        "inference_time": 0.40590596199035645
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999993295729128
          ]
        ],
        "inference_time": 0.513869047164917
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999993295729128
          ]
        ],
        "inference_time": 0.6125123500823975
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999448776502
          ]
        ],
        "inference_time": 0.4213132858276367
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 45,
    "sentence": "He has collaborated with a bevy of big name artists - including Gomez herself, on tracks such as 2023's Single Soon, and his 2019 song I Can\u2019t Get Enough.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.49606847763061523
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.4105958938598633
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.4119875431060791
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.4084444046020508
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.46279025077819824
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.5635478496551514
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.7114846706390381
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.42474794387817383
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.6979727745056152
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.40833401679992676
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 46,
    "sentence": "RULE 4: MOISTURISE  Gina uses organic coconut oil every day on her body, as she admitted \"I have moisturised my entire body every day since I was 19.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9988292720282691
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5081353187561035
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9980716793861388
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.6132874488830566
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9995685894203131
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.42672204971313477
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9975257458812302
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.49210572242736816
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9995685894203131
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4110381603240967
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9988292720282691
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.41132354736328125
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9959276557959932
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.31523704528808594
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9995685894203131
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.38674139976501465
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9988292720282691
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5604488849639893
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9988292720282691
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.46010780334472656
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 47,
    "sentence": "It\u2019s this second group that Barratt is targeting for Ayrton House \u2013 final year medical students and graduate trainees from the surrounding universities, which include Westminster, Middlesex and UCL.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.4119584560394287
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.3523838520050049
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.4842560291290283
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.5433859825134277
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.32379746437072754
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.33739495277404785
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.6098299026489258
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.4042651653289795
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.2968320846557617
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.4200601577758789
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 48,
    "sentence": "This is why the more Osborne slashed public spending in the 2010s, the more money he needed to borrow.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9959180795594925
          ]
        ],
        "inference_time": 0.5011956691741943
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9975130520889396
          ]
        ],
        "inference_time": 0.5197627544403076
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9975107999476589
          ]
        ],
        "inference_time": 0.7098863124847412
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9626555400976607
          ]
        ],
        "inference_time": 0.40378785133361816
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9975107999476589
          ]
        ],
        "inference_time": 0.41051173210144043
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9994387842322136
          ]
        ],
        "inference_time": 0.4238855838775635
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9988173766379367
          ]
        ],
        "inference_time": 0.34670543670654297
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9980614660315511
          ]
        ],
        "inference_time": 0.4221222400665283
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9975107999476589
          ]
        ],
        "inference_time": 0.32879638671875
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9932945411455295
          ]
        ],
        "inference_time": 0.36640214920043945
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 49,
    "sentence": "When your credit card is \u201cmaxed out\u201d, you do indeed need immediately to tighten your belt.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.47711825370788574
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3367640972137451
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4131174087524414
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.89607572555542
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.38198304176330566
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4089362621307373
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3196132183074951
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.42740440368652344
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4110400676727295
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3299367427825928
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 50,
    "sentence": "This was inaccurate.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999996871837232
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.32627367973327637
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999998063873693
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.435396671295166
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999996871837232
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3572871685028076
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999998063873693
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.44152212142944336
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4194824695587158
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5056612491607666
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999998063873693
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5139596462249756
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.999999448776502
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.41217517852783203
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999998063873693
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.35466432571411133
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999998063873693
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5727472305297852
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 51,
    "sentence": "She now has her 26-year-old granddaughter, Eliza Brunero, lodging with her on Wednesdays each week.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.3283367156982422
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.31025123596191406
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.37426066398620605
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.7109150886535645
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.4125981330871582
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.38799524307250977
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.3196408748626709
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.41406893730163574
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.3983325958251953
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.8362948894500732
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 52,
    "sentence": "Wangari is one of 16 Kenyan women who have died allegedly at the hands of their partners since the start of 2024.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.5898077487945557
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.3460104465484619
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.37945055961608887
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.40664243698120117
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.43950533866882324
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.4794642925262451
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.3386960029602051
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.35666584968566895
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 1.3533728122711182
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.42530059814453125
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 53,
    "sentence": "And the Caesar Kunikov's watery demise is just the latest blow to have embarrassed Putin.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.486889123916626
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.40486955642700195
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.510643720626831
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.40633058547973633
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3061966896057129
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.6118454933166504
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.407686710357666
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3386256694793701
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3755812644958496
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4076511859893799
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 54,
    "sentence": "Beks then shared a clip of what happened straight after and continues: \"It was my lunch break and I just dipped out and didn't tell anybody.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9995685894203131
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.8155887126922607
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9997958844284633
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5076181888580322
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9933045403599551
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3047187328338623
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9996639982562401
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.304934024810791
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9980722742833953
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.42429018020629883
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9998411699326909
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.9294407367706299
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.999924830105154
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5304994583129883
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9988297481910151
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.7010269165039062
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9968254321712563
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3036026954650879
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9988297481910151
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5071933269500732
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 55,
    "sentence": "I know you was heartbroken lol.\"",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.244643211364746
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999998063873693
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.49492955207824707
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999998063873693
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5129339694976807
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.7173359394073486
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999998063873693
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5114083290100098
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 3.1775708198547363
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999998063873693
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.40913939476013184
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999996871837232
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5053081512451172
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.9475033283233643
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999998063873693
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.40906715393066406
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 56,
    "sentence": "\u201cI can\u2019t say I add anything to her life but she certainly brightens up mine.\u201d  Brunero begs to differ.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.40580058097839355
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.438354253768921
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.30185747146606445
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5097494125366211
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.47126269340515137
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.6556811332702637
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.30019092559814453
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5197298526763916
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.433363676071167
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3133585453033447
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 57,
    "sentence": "But while Leo Woodall is enjoying a career boost for his portrayal as 'pampered Southern toff' Dexter Mayhew, the 2011 film adaptation had the exact opposite effect for actor Jim Sturgess.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9525720468945272
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4103062152862549
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9914216064945817
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5178256034851074
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9770209115331047
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.36962437629699707
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9975266899964694
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.31339240074157715
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9975266899964694
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5617008209228516
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9980728690648153
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5819637775421143
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9990883562939145
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.30258750915527344
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9975269278253939
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4524073600769043
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9988299864468919
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.36704325675964355
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.998829867377086
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4803488254547119
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 58,
    "sentence": "I scribbled down what I saw and what I felt and the song kind of wrote itself.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.2790045738220215
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3286881446838379
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4079294204711914
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.38326072692871094
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5327754020690918
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4109313488006592
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.40892577171325684
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.40433502197265625
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.41551804542541504
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.30026745796203613
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 59,
    "sentence": "In the first image, the Emmy-nominated actress embraced Blanco, who has a passion for cooking, while he was preparing a platter of meatballs.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.40434694290161133
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.40550756454467773
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.5108211040496826
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.41533827781677246
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999448776502
          ]
        ],
        "inference_time": 0.6149568557739258
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999995679801056
          ]
        ],
        "inference_time": 0.304642915725708
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999995679801056
          ]
        ],
        "inference_time": 0.41110730171203613
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999448776502
          ]
        ],
        "inference_time": 0.41815924644470215
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999995679801056
          ]
        ],
        "inference_time": 0.3002598285675049
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.5039594173431396
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 60,
    "sentence": "It follows their range of Unmanned Surface Vehicles (USVs) which have been hugely successful in Black Sea attacks.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9990789587298193
          ]
        ],
        "inference_time": 0.5432093143463135
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9990818170054289
          ]
        ],
        "inference_time": 0.5236403942108154
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9914002823408152
          ]
        ],
        "inference_time": 0.8002471923828125
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999075151449623
          ]
        ],
        "inference_time": 0.4292478561401367
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9989454761906357
          ]
        ],
        "inference_time": 0.4965541362762451
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9989454761906357
          ]
        ],
        "inference_time": 0.3101465702056885
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9990789587298193
          ]
        ],
        "inference_time": 0.5102155208587646
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9998814441784764
          ]
        ],
        "inference_time": 0.40012025833129883
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9889932741203459
          ]
        ],
        "inference_time": 0.5778284072875977
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999075151449623
          ]
        ],
        "inference_time": 0.557478666305542
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 61,
    "sentence": "But one person who knows exactly what's that like after getting the winning number on a scratchcard has told how they were left bitterly \"disappointed.\"",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5085957050323486
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4110863208770752
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.40904760360717773
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5053393840789795
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.507737398147583
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5123224258422852
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.716447114944458
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.32192516326904297
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.32539868354797363
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.37398791313171387
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 62,
    "sentence": "Selena looked chic in a brown turtleneck, with her hair brushed back into a ponytail, while her other half rocked a white tee, and gold chains.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.991421254704648
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.30821943283081055
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999540332370438
          ]
        ],
        "inference_time": 0.40673279762268066
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9995688277360644
          ]
        ],
        "inference_time": 0.3482515811920166
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.622457215094258
          ]
        ],
        "inference_time": 0.9771466255187988
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.991421254704648
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3064084053039551
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999540332370438
          ]
        ],
        "inference_time": 0.4140644073486328
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9770212518438538
          ]
        ],
        "inference_time": 0.5141892433166504
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9914210229480932
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5123515129089355
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.6791764902159297
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.40462160110473633
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.7310565304584647
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3118464946746826
      }
    ],
    "predicted_label": "ambiguous"
  },
  {
    "sample_idx": 63,
    "sentence": "The refreshing beer-and-fizzy-pop combination bubbled away as a quietly constant \u2013 if not cult \u2013 pub choice for over half my lifetime, then seemed to fizzle out over the past 15 years.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.41078948974609375
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.6071863174438477
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.31204771995544434
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.6134557723999023
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5131046772003174
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4099256992340088
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3751411437988281
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4372267723083496
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5099565982818604
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4041013717651367
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 64,
    "sentence": "Kyiv continues to target Russia\u2019s Black Sea Fleet with great effectDr Bastian Giegerich, IISS security analyst  And Vlad's Black Sea fleet was \"put on the defensive by several events\", which included the sea drone attack in December that \"badly damaged a landing ship off Novorossiysk\".",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999995679801056
          ]
        ],
        "inference_time": 0.4137303829193115
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999993295729128
          ]
        ],
        "inference_time": 1.118586778640747
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999448776502
          ]
        ],
        "inference_time": 0.4102742671966553
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999448776502
          ]
        ],
        "inference_time": 0.5697851181030273
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999993295729128
          ]
        ],
        "inference_time": 0.6779310703277588
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999448776502
          ]
        ],
        "inference_time": 0.5960023403167725
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999993295729128
          ]
        ],
        "inference_time": 0.43648695945739746
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999993295729128
          ]
        ],
        "inference_time": 0.3382561206817627
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999448776502
          ]
        ],
        "inference_time": 0.35783886909484863
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999993295729128
          ]
        ],
        "inference_time": 0.5029458999633789
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 65,
    "sentence": "\u201cIt\u2019s such a bonus \u2013 sometimes I cook for her, sometimes she cooks for me, we play cards, we watch TV and we chat,\u201d Hamilton says.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999995679801056
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.40860629081726074
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999995679801056
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3308851718902588
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999998063873693
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.9002127647399902
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999995679801056
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4029276371002197
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999995679801056
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5171606540679932
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999996871837232
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.40424656867980957
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999995679801056
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3305473327636719
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999995679801056
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.0165655612945557
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999995679801056
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3093447685241699
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999995679801056
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.30567097663879395
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 66,
    "sentence": "She was fairly close to Wangari, and recalls with sadness how she had promised to send her money the following week so she could move to a new flat.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5125176906585693
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.40875816345214844
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.41019248962402344
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4082763195037842
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5148396492004395
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3043045997619629
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5114381313323975
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4070131778717041
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.41317319869995117
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.617445707321167
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 67,
    "sentence": "And it could even be used to gather intelligence on Russian operations.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999987335551229
          ]
        ],
        "inference_time": 0.8158085346221924
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999983759447105
          ]
        ],
        "inference_time": 1.2334372997283936
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999988527586979
          ]
        ],
        "inference_time": 0.5504531860351562
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999987335551229
          ]
        ],
        "inference_time": 0.37592291831970215
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999091165777
          ]
        ],
        "inference_time": 2.947627544403076
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999987335551229
          ]
        ],
        "inference_time": 0.5094811916351318
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999091165777
          ]
        ],
        "inference_time": 0.6133100986480713
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999992103693378
          ]
        ],
        "inference_time": 0.4068105220794678
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999987335551229
          ]
        ],
        "inference_time": 0.5152609348297119
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999992103693378
          ]
        ],
        "inference_time": 0.40462684631347656
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 68,
    "sentence": "\"I'm not really a gambler - it was a $10k (\u00a37.9k) a week for life scratch card and I scratched it off.\"",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.999941398309622
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.40487146377563477
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999720323248966
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.41138386726379395
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999993295729128
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.30610108375549316
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999249493054897
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5901679992675781
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.999999448776502
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.33067774772644043
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999831180163555
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4060039520263672
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999032560969745
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3061857223510742
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999831180163555
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.30583858489990234
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999831180163555
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4053633213043213
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999831180163555
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.4362356662750244
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 69,
    "sentence": "When it was an Emma and Dex day I felt good about it.'",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.401867151260376
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3000791072845459
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.41353321075439453
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5113563537597656
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.6116890907287598
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.412764310836792
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5115194320678711
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.6076130867004395
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.517888069152832
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5102252960205078
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 70,
    "sentence": "RULE 10: BE HAPPY  She then advised: \u201cLike the song don\u2019t worry be happy and create a life of meaningful memories.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4373161792755127
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.8149299621582031
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.40619802474975586
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5122981071472168
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4056389331817627
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.40633654594421387
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5097379684448242
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.34659433364868164
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3623988628387451
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.6156964302062988
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 71,
    "sentence": "But perhaps most impressively, Ukrainian forces managed to blow up Russia's flagship vessel - the Moskva - in April 2022.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999992103693378
          ]
        ],
        "inference_time": 0.5064220428466797
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999992103693378
          ]
        ],
        "inference_time": 0.9199764728546143
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999448776502
          ]
        ],
        "inference_time": 0.5111174583435059
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999986143516758
          ]
        ],
        "inference_time": 0.5231552124023438
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999448776502
          ]
        ],
        "inference_time": 0.40048718452453613
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999989719621736
          ]
        ],
        "inference_time": 0.3048892021179199
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999448776502
          ]
        ],
        "inference_time": 0.4059121608734131
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999998137537802
          ]
        ],
        "inference_time": 0.41136670112609863
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999448776502
          ]
        ],
        "inference_time": 0.4062187671661377
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999448776502
          ]
        ],
        "inference_time": 0.34540343284606934
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 72,
    "sentence": "But Michelle evidently didn't hold the blunder against him; the couple have been married since 2015 after meeting in late 2012.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999995679801056
          ]
        ],
        "inference_time": 0.8739697933197021
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999995679801056
          ]
        ],
        "inference_time": 0.4043886661529541
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999448776502
          ]
        ],
        "inference_time": 0.3920423984527588
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.33271288871765137
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999995679801056
          ]
        ],
        "inference_time": 0.45232319831848145
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999448776502
          ]
        ],
        "inference_time": 0.33429646492004395
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999448776502
          ]
        ],
        "inference_time": 0.4303579330444336
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.4132235050201416
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999995679801056
          ]
        ],
        "inference_time": 0.3743095397949219
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999995679801056
          ]
        ],
        "inference_time": 0.38826894760131836
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 73,
    "sentence": "Vigils, dubbed \u201cDark Valentine\u201d, were held across Kenya this week after a month in which more than a dozen women have been killed, allegedly by their partners.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.28781700134277344
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.2972533702850342
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999998063873693
          ]
        ],
        "inference_time": 0.3896925449371338
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.6101226806640625
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.24756097793579102
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.3353307247161865
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.5337061882019043
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.37014055252075195
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999998063873693
          ]
        ],
        "inference_time": 0.655846118927002
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999995679801056
          ]
        ],
        "inference_time": 0.36803531646728516
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 74,
    "sentence": "The radio presenter revealed the Fool Me Once star rumbled his porky pies the next day and fumed: 'Mark, you're a liar!'",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999998063873693
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4499373435974121
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999998063873693
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.40988969802856445
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999998063873693
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.8670005798339844
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999998063873693
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3832211494445801
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.411790132522583
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999996871837232
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.46280741691589355
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999998063873693
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4552938938140869
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999998063873693
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4112119674682617
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999998063873693
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.6108648777008057
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999998063873693
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.40929388999938965
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 75,
    "sentence": "The snap comes after Selena surprised fans on Monday with a very racy photo of Benny grabbing her cleavage  Gomez has been linked with Blanco, a musical artist, producer, and songwriter, since last year  In the first image from the post, the Emmy-nominated actress embraced Blanco, who has a passion for cooking, while he was preparing a platter of meatballs  Blanco was posed behind Gomez as they relaxed in the kitchen area of a home alongside friends including The Bear actor Matty Matheson  On Sunday, the couple was seen kissing one another while posed against a velvet red couch in a shot Gomez posted to Instagram  Gomez and Blanco have frequently shared their light-hearted and romantic moments via Instagram in the early days of their relationship.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999448776502
          ]
        ],
        "inference_time": 0.3042571544647217
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999978991308366
          ]
        ],
        "inference_time": 0.406658411026001
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9998751344793849
          ]
        ],
        "inference_time": 1.8457767963409424
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999978991308366
          ]
        ],
        "inference_time": 0.4034552574157715
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999857404571098
          ]
        ],
        "inference_time": 0.4062979221343994
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999448776502
          ]
        ],
        "inference_time": 0.5127787590026855
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999821644040676
          ]
        ],
        "inference_time": 0.39732885360717773
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999710787231805
          ]
        ],
        "inference_time": 0.42548346519470215
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999821644040676
          ]
        ],
        "inference_time": 0.5116307735443115
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999839739707646
          ]
        ],
        "inference_time": 0.6129310131072998
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 76,
    "sentence": "November 17, 2023  Sir, your article \u2018BBC crisis over \u00a31.7bn pension bill for stars\u2019 (4/11/2023) is incorrect and significantly inflates the BBC\u2019s obligation to fund its defined benefit pension scheme shortfall and employer contribution rate.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9820013635649378
          ]
        ],
        "inference_time": 0.40655970573425293
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9984930583898546
          ]
        ],
        "inference_time": 0.3091771602630615
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9980681138443307
          ]
        ],
        "inference_time": 0.7095499038696289
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9968174965300372
          ]
        ],
        "inference_time": 0.40648746490478516
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999524836614722
          ]
        ],
        "inference_time": 0.6150968074798584
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9990843179887565
          ]
        ],
        "inference_time": 0.40915989875793457
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9890047028448071
          ]
        ],
        "inference_time": 0.409853458404541
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9947724853203669
          ]
        ],
        "inference_time": 0.6099543571472168
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9997931433091487
          ]
        ],
        "inference_time": 0.5145704746246338
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9998744193568644
          ]
        ],
        "inference_time": 0.5111010074615479
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 77,
    "sentence": "The crime currently falls under homicide provisions, which rights groups say do not account for the unequal power relations between men and women that drive and characterise the killings.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9996637599177984
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3745391368865967
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9997381024463875
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4409027099609375
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9990878798915754
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.7559046745300293
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9997381024463875
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.46816396713256836
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9996637599177984
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4079751968383789
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9994460479301669
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4142470359802246
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9994461670734273
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.41356730461120605
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9997960036134276
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4068126678466797
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9996637599177984
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3070054054260254
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999412791109633
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.8161771297454834
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 78,
    "sentence": "Fr\u00fch Natur Radler  2.5%; The Real Ale Store, \u00a31.85 for 500ml; Hop Burns & Black, \u00a32.30  If grapefruit is not your bag, seek out this lemony-fresh delight from Fr\u00fch.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4095172882080078
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.41227054595947266
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.6115448474884033
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.6165745258331299
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5099449157714844
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.6142563819885254
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.509209156036377
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5100018978118896
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.42528200149536133
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4945647716522217
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 79,
    "sentence": "Following on from this, Jim went on to star in the 2012 adaptation of David Mitchell's Cloud Atlas, which also divided audiences.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9859101360236427
          ]
        ],
        "inference_time": 0.5179548263549805
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9998393821662539
          ]
        ],
        "inference_time": 0.49855923652648926
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.994763394994545
          ]
        ],
        "inference_time": 0.4073762893676758
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9859101360236427
          ]
        ],
        "inference_time": 0.4066934585571289
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9996619724685795
          ]
        ],
        "inference_time": 0.5145723819732666
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9975208817858084
          ]
        ],
        "inference_time": 0.4093921184539795
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9980673999706413
          ]
        ],
        "inference_time": 0.4111654758453369
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9914135208976429
          ]
        ],
        "inference_time": 0.9168081283569336
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9398825060978402
          ]
        ],
        "inference_time": 0.5107181072235107
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9525669653022776
          ]
        ],
        "inference_time": 0.9201838970184326
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 80,
    "sentence": "Limit your spending and you have limited your income too.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9859338818892712
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.32842206954956055
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9980716793861388
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5142748355865479
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9933055944970848
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5096383094787598
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9992894894950091
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.40939807891845703
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9975264593674983
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.7166991233825684
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9980716793861388
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5122401714324951
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9992894894950091
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4059481620788574
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9994466435884355
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3216876983642578
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9046442847489546
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3937373161315918
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9525670717599378
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.818734884262085
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 81,
    "sentence": "Unlike some of the other lemon offerings out there, this isn\u2019t soapy at all and has a very natural lemon flavouring.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4068412780761719
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.40631842613220215
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5760540962219238
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.451505184173584
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4081454277038574
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5099403858184814
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.511873722076416
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.41387367248535156
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.40979552268981934
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4089016914367676
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 82,
    "sentence": "Blanco was past rumored to be in a romance with Elsie Hewitt, a 27-year-old model-actress from London (who has since been linked with Jason Sudeikis, 48).",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999989719621736
          ]
        ],
        "inference_time": 0.33883047103881836
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999984951481292
          ]
        ],
        "inference_time": 0.37883758544921875
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999091165777
          ]
        ],
        "inference_time": 0.4199349880218506
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999091165777
          ]
        ],
        "inference_time": 0.40433621406555176
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999992103693378
          ]
        ],
        "inference_time": 0.6034228801727295
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999091165777
          ]
        ],
        "inference_time": 0.40853309631347656
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999989719621736
          ]
        ],
        "inference_time": 0.5066778659820557
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999986143516758
          ]
        ],
        "inference_time": 0.5139503479003906
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999448776502
          ]
        ],
        "inference_time": 0.4002695083618164
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999987335551229
          ]
        ],
        "inference_time": 0.5072321891784668
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 83,
    "sentence": "The Schedule of Contributions agreed with the Scheme Trustee as part of the 2022 actuarial valuation states that the employer contribution rate for the defined benefit pension scheme is currently set to 30% of members\u2019 pensionable salaries.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.40178966522216797
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999995679801056
          ]
        ],
        "inference_time": 0.4194221496582031
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999995679801056
          ]
        ],
        "inference_time": 0.4070255756378174
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.512021541595459
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999995679801056
          ]
        ],
        "inference_time": 0.41004085540771484
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.40833139419555664
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999995679801056
          ]
        ],
        "inference_time": 0.4115433692932129
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999995679801056
          ]
        ],
        "inference_time": 0.5111925601959229
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.8162429332733154
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 1.1271696090698242
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 84,
    "sentence": "So I wanted more.'",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5097293853759766
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999998063873693
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.118126630783081
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.406536340713501
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.6194238662719727
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4065265655517578
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.6101212501525879
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999998063873693
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4137437343597412
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999998063873693
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.45603013038635254
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999998063873693
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.46544504165649414
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5115396976470947
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 85,
    "sentence": "Nearly half of Republicans and right-leaning independents said the US was providing too much aid to Ukraine, according to a survey by the Pew Research Center conducted late last year.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.40433573722839355
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.41092371940612793
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.4062049388885498
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.6164751052856445
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.7197496891021729
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.5099618434906006
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.4091618061065674
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.4012484550476074
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.41280531883239746
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999996871837232
          ]
        ],
        "inference_time": 0.4112567901611328
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 86,
    "sentence": "Meanwhile, some arch-conservatives suggested it was time for McConnell to step down.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9992899659353167
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.40768933296203613
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.999796122783857
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.40703296661376953
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9995690660227811
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5116057395935059
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9997962419688495
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4049203395843506
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9992899659353167
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.41143155097961426
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999543908303483
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.2287898063659668
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9992899659353167
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4072840213775635
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9994467627899424
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3098320960998535
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9995690660227811
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3780505657196045
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9994467627899424
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.33985447883605957
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 87,
    "sentence": "\u201cI maintain a healthy weight that preserves my facial structure and collagen.\u201d",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.2958979606628418
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.41097211837768555
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4075627326965332
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.9206438064575195
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.7110903263092041
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.41223788261413574
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4062681198120117
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5095009803771973
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.9199802875518799
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.40869951248168945
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 88,
    "sentence": "Later in a campaign speech, Trump rattled American allies in Europe when he claimed that he would encourage Russia to attack Nato allies who did not pay enough to maintain the security alliance.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999993295729128
          ]
        ],
        "inference_time": 0.4024789333343506
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999993295729128
          ]
        ],
        "inference_time": 0.6175863742828369
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999448776502
          ]
        ],
        "inference_time": 0.5122840404510498
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999448776502
          ]
        ],
        "inference_time": 0.39991283416748047
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999992103693378
          ]
        ],
        "inference_time": 0.6203410625457764
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999993295729128
          ]
        ],
        "inference_time": 0.5153861045837402
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999448776502
          ]
        ],
        "inference_time": 0.4057779312133789
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999448776502
          ]
        ],
        "inference_time": 0.4143977165222168
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999993295729128
          ]
        ],
        "inference_time": 0.40143680572509766
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999993295729128
          ]
        ],
        "inference_time": 0.415973424911499
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 89,
    "sentence": "Mark made the cheeky confession on-air while hosting his Heart FM radio programme on Monday, admitting he got 'clocked' by Michelle when she found the restaurant takeaway bag in the bin.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999993295729128
          ]
        ],
        "inference_time": 0.5422720909118652
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999995679801056
          ]
        ],
        "inference_time": 0.7829320430755615
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999448776502
          ]
        ],
        "inference_time": 0.7172751426696777
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999448776502
          ]
        ],
        "inference_time": 0.5081579685211182
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999448776502
          ]
        ],
        "inference_time": 0.4068186283111572
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999993295729128
          ]
        ],
        "inference_time": 0.5091955661773682
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999993295729128
          ]
        ],
        "inference_time": 0.5144906044006348
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999448776502
          ]
        ],
        "inference_time": 0.506443977355957
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999448776502
          ]
        ],
        "inference_time": 0.5115220546722412
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999448776502
          ]
        ],
        "inference_time": 0.47377490997314453
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 90,
    "sentence": "\u201cIt has my facial expressions \u2013 my eyes, lips, hair, arms, legs, chest and butt, all dimples and textures fully integrated.\u201d  Fabulous will pay for your exclusive stories.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999829988145424
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.68021559715271
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999721515254024
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.8641989231109619
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999866940728078
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3409595489501953
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999829988145424
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.6007077693939209
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999866940728078
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3422567844390869
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999782307609466
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.41094207763671875
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999896741288624
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5196211338043213
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999721515254024
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5023171901702881
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999895549271773
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.37807655334472656
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999866940728078
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4003868103027344
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 91,
    "sentence": "Since the 2022 valuation, funding has improved.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999448776502
          ]
        ],
        "inference_time": 0.4996006488800049
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999448776502
          ]
        ],
        "inference_time": 2.251683235168457
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999448776502
          ]
        ],
        "inference_time": 0.5113015174865723
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999995679801056
          ]
        ],
        "inference_time": 0.4056065082550049
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999993295729128
          ]
        ],
        "inference_time": 0.4094521999359131
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999995679801056
          ]
        ],
        "inference_time": 0.4100825786590576
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999995679801056
          ]
        ],
        "inference_time": 0.3062751293182373
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999995679801056
          ]
        ],
        "inference_time": 0.5124788284301758
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999999448776502
          ]
        ],
        "inference_time": 0.5109081268310547
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999995679801056
          ]
        ],
        "inference_time": 1.532292127609253
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 92,
    "sentence": "He said: 'At a very young age, the local theatre in my town went to local schools to cast a load of kids to be in a professional production of Wind In The Willows.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999981806799012
          ]
        ],
        "inference_time": 0.6137335300445557
      },
      {
        "repetition": 1,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9998733466522176
          ]
        ],
        "inference_time": 0.513117790222168
      },
      {
        "repetition": 2,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999938462231682
          ]
        ],
        "inference_time": 0.6134390830993652
      },
      {
        "repetition": 3,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999766811478773
          ]
        ],
        "inference_time": 0.5120828151702881
      },
      {
        "repetition": 4,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999926541947074
          ]
        ],
        "inference_time": 0.5102896690368652
      },
      {
        "repetition": 5,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999964686909203
          ]
        ],
        "inference_time": 0.6115303039550781
      },
      {
        "repetition": 6,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.999997303114013
          ]
        ],
        "inference_time": 0.5164086818695068
      },
      {
        "repetition": 7,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999909853565219
          ]
        ],
        "inference_time": 0.3418538570404053
      },
      {
        "repetition": 8,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999883629029224
          ]
        ],
        "inference_time": 0.6805422306060791
      },
      {
        "repetition": 9,
        "generated_text": "objective",
        "token_probs": [
          [
            "objective",
            0.9999959918780695
          ]
        ],
        "inference_time": 0.5114240646362305
      }
    ],
    "predicted_label": "objective"
  },
  {
    "sample_idx": 93,
    "sentence": "RULE 9: LOVE  According to Gina, you should love yourself, love your life, love others.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.1252222061157227
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.30526065826416016
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5120928287506104
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.612621545791626
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4872584342956543
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5348320007324219
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.40998363494873047
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3076026439666748
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.9220638275146484
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.40386438369750977
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 94,
    "sentence": "Anne Hathaway is not it.",
    "true_label": 1,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.40544581413269043
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4084341526031494
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5158970355987549
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.6116254329681396
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3055386543273926
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5098495483398438
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4132113456726074
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.002896785736084
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.1429593563079834
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5164105892181396
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 95,
    "sentence": "As she watches her granddaughters play, Wairimu wonders if things could have played out differently.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5099916458129883
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.40291810035705566
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 3.45375394821167
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.43246984481811523
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5189652442932129
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4061877727508545
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5097653865814209
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.6159296035766602
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4072585105895996
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.40734291076660156
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 96,
    "sentence": "RULE 5: ATTITUDE IS EVERYTHING  Gina stressed the importance of \"thinking yourself young\" as she suggested: \"You are what you think.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4097282886505127
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5140230655670166
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.6179351806640625
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.3279433250427246
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.41443562507629395
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.6125986576080322
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.40758705139160156
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5137407779693604
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3441343307495117
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4030306339263916
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 97,
    "sentence": "However, Variety wasn't so gushing - and said the drama 'should have stayed in quarantine'.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3735678195953369
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.41275691986083984
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5111486911773682
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.32041382789611816
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3961782455444336
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.6078543663024902
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4146397113800049
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4058225154876709
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5089561939239502
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 1.1326570510864258
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 98,
    "sentence": "The Ohio senator JD Vance, another Trump loyalist, claimed the effort to replenish Ukraine\u2019s war chest was a \u201cplot\u201d by the Republican establishment to \u201cstop the election of Donald Trump\u201d.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999995679801056
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.3895571231842041
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5279948711395264
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4083569049835205
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4143369197845459
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.37493133544921875
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999996871837232
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.44098377227783203
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999996871837232
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.40468764305114746
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999996871837232
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.41004014015197754
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.32241201400756836
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            0.9999998063873693
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.6996476650238037
      }
    ],
    "predicted_label": "subjective"
  },
  {
    "sample_idx": 99,
    "sentence": "It\u2019s made me realise how much we take older people for granted.\u201d  Both Brunero and Hamilton can see how mutually beneficial a multigenerational community such as Ayrton House could be.",
    "true_label": 0,
    "repetitions": [
      {
        "repetition": 0,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5080068111419678
      },
      {
        "repetition": 1,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.409895658493042
      },
      {
        "repetition": 2,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5103380680084229
      },
      {
        "repetition": 3,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5096011161804199
      },
      {
        "repetition": 4,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.41048765182495117
      },
      {
        "repetition": 5,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.2865169048309326
      },
      {
        "repetition": 6,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.5332925319671631
      },
      {
        "repetition": 7,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.40971899032592773
      },
      {
        "repetition": 8,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.47029995918273926
      },
      {
        "repetition": 9,
        "generated_text": "subjective",
        "token_probs": [
          [
            "subject",
            1.0
          ],
          [
            "ive",
            1.0
          ]
        ],
        "inference_time": 0.4582672119140625
      }
    ],
    "predicted_label": "subjective"
  }
]